{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Project.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABCcJUJaymgp",
        "outputId": "64e4a179-5074-4e9f-9e99-a2ef90133361"
      },
      "source": [
        "!which pip\n",
        "!pwd"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/global/software/jupyterhub-spark/anaconda3/bin/pip\n",
            "/home/gregory.slowski\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_3GoCVhymgq",
        "outputId": "137c6bc4-3cf1-4c6a-efb6-6cfa2ca2c793"
      },
      "source": [
        "import os\n",
        "import atexit\n",
        "import sys\n",
        "import time\n",
        "import pyspark\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "import findspark\n",
        "from sparkhpc import sparkjob\n",
        "\n",
        "#Exit handler to clean up the Spark cluster if the script exits or crashes\n",
        "def exitHandler(sj,sc):\n",
        "    try:\n",
        "        print('Trapped Exit cleaning up Spark Context')\n",
        "        sc.stop()\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        print('Trapped Exit cleaning up Spark Job')\n",
        "        sj.stop()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "#Parameters for the Spark cluster\n",
        "nodes=1\n",
        "tasks_per_node=4\n",
        "memory_per_task=4096 #4 gig per process, adjust accordingly\n",
        "# Please estimate walltime carefully to keep unused Spark clusters from sitting \n",
        "# idle so that others may use the resources.\n",
        "walltime=\"15:00\" #12 hours\n",
        "#os.environ['SBATCH_PARTITION']='cpu2019' #Set the appropriate ARC partition\n",
        "\n",
        "sj = sparkjob.sparkjob(\n",
        "     ncores=nodes*tasks_per_node,\n",
        "     cores_per_executor=tasks_per_node,\n",
        "     memory_per_core=memory_per_task,\n",
        "     walltime=walltime\n",
        "    )\n",
        "\n",
        "sj.wait_to_start()\n",
        "time.sleep(60)\n",
        "\n",
        "sc = sj.start_spark()\n",
        "\n",
        "#Register the exit handler                                                                                                     \n",
        "atexit.register(exitHandler,sj,sc)\n",
        "\n",
        "#You need this line if you want to use SparkSQL\n",
        "sqlCtx=SQLContext(sc)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:sparkhpc.sparkjob:Submitted batch job 12529\n",
            "\n",
            "INFO:sparkhpc.sparkjob:Submitted cluster 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLsp9nYpymgq"
      },
      "source": [
        "sqlContext = sqlCtx\n",
        "spark = sqlCtx"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTe06vP5ymgr",
        "outputId": "0009594a-ed04-4997-9648-408863d6aa64"
      },
      "source": [
        "#!pip install --upgrade pip\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import udf"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /global/software/jupyterhub-spark/anaconda3/lib/python3.7/site-packages (3.3)\r\n",
            "Requirement already satisfied: six in /global/software/jupyterhub-spark/anaconda3/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
            "\u001b[33mYou are using pip version 10.0.1, however version 21.3.1 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /home/gregory.slowski/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BX17T63eymgr"
      },
      "source": [
        "def tokenize1(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    return words  \n",
        "tokenize_word = udf(lambda x: tokenize1(x)  , ArrayType(StringType()))"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIlta-GJymgr"
      },
      "source": [
        "def tokenize2(text):\n",
        "    sents = nltk.sent_tokenize(text)\n",
        "    return sents  \n",
        "tokenize_sent = udf(lambda x: tokenize2(x)  , ArrayType(StringType()))"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQ7KXv5Symgr",
        "outputId": "f077a5da-63de-4108-93d0-7a7918c11913"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_en = stopwords.words('english')\n",
        "def remove_stopwords1(word_list):\n",
        "    filtered_words = [word for word in word_list if word not in stop_en]\n",
        "    return filtered_words\n",
        "remove_stopwords = udf(lambda x: remove_stopwords1(x) , ArrayType(StringType()))"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/gregory.slowski/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /home/gregory.slowski/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RToRXi8fymgr"
      },
      "source": [
        "def remove_noise1(word_list):\n",
        "    filtered_words = [word for word in word_list if word.isalnum() and len(word)>2]\n",
        "    return filtered_words\n",
        "remove_noise = udf(lambda x: remove_noise1(x) , ArrayType(StringType()))"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91Olkdiwymgs"
      },
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "def stem1(word_list):\n",
        "    snowball = SnowballStemmer(language='english')\n",
        "    stemmed_words = [snowball.stem(word) for word in word_list]\n",
        "    return stemmed_words\n",
        "stem = udf(lambda x: stem1(x) , ArrayType(StringType()))"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJUre_G9ymgs",
        "outputId": "88a8c256-be12-4424-d171-b62280e6c6ed"
      },
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "def sentiment1(text):\n",
        "  sia = SentimentIntensityAnalyzer()\n",
        "  return sia.polarity_scores(text)['compound']\n",
        "sentiment = udf(lambda x: sentiment1(x) , FloatType())"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     /home/gregory.slowski/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGFXg2gI5GnA",
        "outputId": "92ca563f-e686-444f-b1c7-8d6e4e4aea6f"
      },
      "source": [
        "#area for testing functions:\n",
        "\n",
        "mylist = ['GeeksforGeeks', \",\", 'is', 'a', 'portal', 'for', 'geeks']\n",
        "alphanum_words = [word for word in mylist if word.isalnum()]\n",
        "N = len(alphanum_words)\n",
        "punctuation_words = [word for word in mylist if not(word.isalnum())]\n",
        "num_punctuation_words = len(punctuation_words)\n",
        "ratio_punctuation_to_total = num_punctuation_words / N\n",
        "ratio_punctuation_to_total"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.16666666666666666"
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2F9Z5-x1MQS"
      },
      "source": [
        "#greg\n",
        "#create udf for adding length of reviews by word count for all alphanumeric \"words\"\n",
        "\n",
        "def get_length1(word_list):\n",
        "  alphanum_words = [word for word in word_list if word.isalnum()]\n",
        "  N = len(alphanum_words)\n",
        "  return N\n",
        "\n",
        "get_length = udf(lambda x: get_length1(x), IntegerType())"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0AoKhsg2xGr"
      },
      "source": [
        "#greg\n",
        "#create udf for adding average word length by review\n",
        "\n",
        "def get_average_word_length1(word_list):\n",
        "  alphanum_word_lengths = [len(word) for word in word_list if word.isalnum()]\n",
        "  avg_word_len = sum(alphanum_word_lengths)/len(alphanum_word_lengths)\n",
        "  return avg_word_len\n",
        "\n",
        "get_average_word_length = udf(lambda x: get_average_word_length1(x), FloatType())"
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKhA9adF4h2f"
      },
      "source": [
        "#greg\n",
        "#create udf for ratio of capitalized characters to total characters (n), with no white spaces\n",
        "\n",
        "def capital_ratio1(word_list):\n",
        "  s = \"\".join(word_list)\n",
        "  n = len(s)\n",
        "  cap_n = len([char for char in s if char.isupper()])\n",
        "  ratio_cap_to_total = cap_n / n\n",
        "  return ratio_cap_to_total\n",
        "\n",
        "capital_ratio = udf(lambda x: capital_ratio1(x), FloatType())\n"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD1h-42o8Tff"
      },
      "source": [
        "#greg\n",
        "#create udf for ratio of long words (>5 letters) to total words (N)\n",
        "\n",
        "def long_word_ratio1(word_list):\n",
        "  alphanum_words = [word for word in word_list if word.isalnum()]\n",
        "  N = len(alphanum_words)\n",
        "  long_words = [word for word in alphanum_words if len(word) > 5]\n",
        "  long_words_count = len(long_words)\n",
        "  ratio_long_to_total = long_words_count / N\n",
        "  return ratio_long_to_total\n",
        "\n",
        "long_word_ratio = udf(lambda x: long_word_ratio1(x), FloatType())"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGml4bBK90Fu"
      },
      "source": [
        "#greg\n",
        "#create udf for ratio of words after stop word removal vs total words\n",
        "\n",
        "stop_en = stopwords.words('english')\n",
        "\n",
        "def filtered_word_ratio1(word_list):\n",
        "  alphanum_words = [word for word in word_list if word.isalnum()]\n",
        "  N = len(alphanum_words)\n",
        "  non_filtered_words = [word for word in alphanum_words if word not in stop_en]\n",
        "  num_non_filtered_words = len(non_filtered_words)\n",
        "  ratio_non_filtered_to_total = num_non_filtered_words / N\n",
        "  return ratio_non_filtered_to_total\n",
        "\n",
        "filtered_word_ratio = udf(lambda x: filtered_word_ratio1(x), FloatType())\n"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL1BEle3_XSS"
      },
      "source": [
        "#greg\n",
        "#create udf for ratio of punctuation \"words\" vs total alphanumeric words\n",
        "\n",
        "def punctuation_word_ratio1(word_list):\n",
        "  alphanum_words = [word for word in word_list if word.isalnum()]\n",
        "  N = len(alphanum_words)\n",
        "  punctuation_words = [word for word in word_list if not(word.isalnum())]\n",
        "  num_punctuation_words = len(punctuation_words)\n",
        "  ratio_punctuation_to_total = num_punctuation_words / N\n",
        "  return ratio_punctuation_to_total\n",
        "\n",
        "punctuation_word_ratio = udf(lambda x: punctuation_word_ratio1(x), FloatType())"
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTMzKHxCymgs"
      },
      "source": [
        "from pyspark.sql.functions import lower\n",
        "def process_data(df):\n",
        "  dfText = df.select(\"Index\",\"Review\" ,\"polarity\",\"real_fake\", tokenize_word(\"Review\").alias(\"tokenized_words\"))\n",
        "  dfText = dfText.withColumn(\"sentiment\" ,sentiment(\"Review\"))\n",
        "  dfText = dfText.withColumn(\"tokenized_sents\" ,tokenize_sent(\"Review\"))\n",
        "  dfText = dfText.withColumn(\"no_stopwords\", remove_stopwords(\"tokenized_words\"))\n",
        "  dfText = dfText.withColumn(\"no_noise\", remove_noise(\"no_stopwords\"))\n",
        "  dfText = dfText.withColumn(\"stemmed\", stem(\"no_noise\"))\n",
        "  #dfText = dfText.select('*', F.concat_ws(\"_\",\"real_fake\",\"polarity\").alias(\"target\"))\n",
        "  dfText = dfText.withColumn(\"length_in_words\", get_length(\"tokenized_words\"))\n",
        "  dfText = dfText.withColumn(\"average_word_length\", get_average_word_length(\"tokenized_words\"))\n",
        "  dfText = dfText.withColumn(\"capital_char_ratio\", capital_ratio(\"tokenized_words\"))\n",
        "  dfText = dfText.withColumn(\"long_word_ratio\", long_word_ratio(\"tokenized_words\"))\n",
        "  dfText = dfText.withColumn(\"non_stop_word_ratio\", filtered_word_ratio(\"tokenized_words\"))\n",
        "  dfText = dfText.withColumn(\"punctuation_ratio\", punctuation_word_ratio(\"tokenized_words\"))\n",
        "\n",
        "  return dfText"
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4N3PTiiymgs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "9bf7b646-84b4-4267-9fef-e96b760c812b"
      },
      "source": [
        "df_raw_old = spark.read.option(\"escape\",\"\\\"\").option(\"header\",True).csv(\"Original_data.csv\")\n",
        "df_old_all = process_data(df_raw_old)\n",
        "#df_test = df_test_all.select(\"Review\",\"stemmed\",\"sentiment\",\"target\",\"real_fake\")\n",
        "df_old = df_old_all.select(\"Review\",\"stemmed\",\"sentiment\",\"polarity\", \"length_in_words\", \"average_word_length\", \\\n",
        "                             \"capital_char_ratio\", \"long_word_ratio\", \"non_stop_word_ratio\", \"punctuation_ratio\", \"real_fake\")\n",
        "\n",
        "old_train_split, old_test_split = df_old.randomSplit(weights = [0.80, 0.20], seed = 1)\n",
        "old_train_split.cache()\n",
        "old_test_split.cache()\n",
        "\n",
        "df_raw_new = spark.read.option(\"escape\",\"\\\"\").option(\"header\",True).csv(\"Hotel_Reviews_Calgary.csv\")\n",
        "df_new_all = process_data(df_raw_new)\n",
        "#df_train = df_train.select(\"Review\",\"stemmed\",\"sentiment\",\"target\",\"real_fake\")\n",
        "df_new = df_new_all.select(\"Review\",\"stemmed\",\"sentiment\",\"polarity\", \"length_in_words\", \"average_word_length\", \\\n",
        "                             \"capital_char_ratio\", \"long_word_ratio\", \"non_stop_word_ratio\", \"punctuation_ratio\", \"real_fake\")\n",
        "\n",
        "new_train_split, new_test_split = df_new.randomSplit(weights = [0.80, 0.20], seed = 1)\n",
        "new_train_split.cache()\n",
        "new_test_split.cache()\n",
        "\n",
        "#df_train.display()\n",
        "combined_train = old_train_split.union(new_train_split)\n",
        "combined_test = old_test_split.union(new_test_split)\n",
        "\n",
        "combined_train.cache()\n",
        "combined_test.cache()"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-193-1670c8d00e63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_raw_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"escape\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\\\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Original_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_old_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_raw_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#df_test = df_test_all.select(\"Review\",\"stemmed\",\"sentiment\",\"target\",\"real_fake\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_old_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Review\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"stemmed\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"sentiment\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"polarity\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"length_in_words\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"average_word_length\"\u001b[0m\u001b[0;34m,\u001b[0m                              \u001b[0;34m\"capital_char_ratio\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"long_word_ratio\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"non_stop_word_ratio\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"punctuation_ratio\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"real_fake\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/global/software/jupyterhub-spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue)\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/global/software/jupyterhub-spark/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/global/software/jupyterhub-spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/global/software/jupyterhub-spark/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o792947.csv.\n: java.lang.OutOfMemoryError: Java heap space\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhemXRN6ymgt"
      },
      "source": [
        "print((combined_train.count(), len(combined_train.columns)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk1XLB55ymgt"
      },
      "source": [
        "combined_train.show(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fi-EzwaK2vxL"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGYen5-symgt"
      },
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, IndexToString, StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "columns_to_be_scaled = ['length_in_words', 'average_word_length', 'capital_char_ratio', 'long_word_ratio', 'non_stop_word_ratio', 'punctuation_ratio']\n",
        "assemblers = [VectorAssembler(inputCols=[col], outputCol=col + \"_vec\") for col in columns_to_be_scaled]\n",
        "scalers  = [StandardScaler(inputCol=col + \"_vec\", outputCol=col + \"_scaled\") for col in columns_to_be_scaled]\n",
        "scaling_pipeline = Pipeline(stages=assemblers + scalers)\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"stemmed\", outputCol=\"rawFeatures\", numFeatures=20000)\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"TF_IDF\", minDocFreq=2)\n",
        "label_strIdx1 = StringIndexer(inputCol=\"polarity\", outputCol=\"polarity_idx\")\n",
        "assembles = VectorAssembler(inputCols = ['TF_IDF','sentiment','polarity_idx', 'length_in_words_scaled', 'average_word_length_scaled', 'capital_char_ratio_scaled', 'long_word_ratio_scaled', 'non_stop_word_ratio_scaled', 'punctuation_ratio_scaled'],outputCol=\"features\")\n",
        "label_strIdx2 = StringIndexer(inputCol=\"real_fake\", outputCol=\"label\")\n",
        "lr = LogisticRegression(regParam = 0.3)\n",
        "label_idxStr = IndexToString(inputCol = \"label\", outputCol = \"article_class\")\n",
        "\n",
        "pipeline = Pipeline(stages=[scaling_pipeline, hashingTF, idf,label_strIdx1, assembles, label_strIdx2, lr ,label_idxStr])\n"
      ],
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH6rivzgymgu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "d199e11d-1e4c-42f4-c75c-cd29d12fecdd"
      },
      "source": [
        "model = pipeline.fit(train_split)"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-192-d5ae300b068f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/global/software/jupyterhub-spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/global/software/jupyterhub-spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/global/software/jupyterhub-spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/global/software/jupyterhub-spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/global/software/jupyterhub-spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/global/software/jupyterhub-spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/global/software/jupyterhub-spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/global/software/jupyterhub-spark/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/global/software/jupyterhub-spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/global/software/jupyterhub-spark/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o792825.fit.\n: java.lang.OutOfMemoryError: Java heap space\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqeM-MJPymgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be566194-6ffb-4f96-bda4-91f920c56a0a"
      },
      "source": [
        "pred = model.transform(test_split)\n",
        "pred_original = model.transform(df_test)\n",
        "pred.cache()\n",
        "pred_original.cache()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Review: string, stemmed: array<string>, sentiment: float, polarity: string, length_in_words: int, average_word_length: float, capital_char_ratio: float, long_word_ratio: float, non_stop_word_ratio: float, punctuation_ratio: float, real_fake: string, length_in_words_vec: vector, average_word_length_vec: vector, capital_char_ratio_vec: vector, long_word_ratio_vec: vector, non_stop_word_ratio_vec: vector, punctuation_ratio_vec: vector, length_in_words_scaled: vector, average_word_length_scaled: vector, capital_char_ratio_scaled: vector, long_word_ratio_scaled: vector, non_stop_word_ratio_scaled: vector, punctuation_ratio_scaled: vector, rawFeatures: vector, TF_IDF: vector, polarity_idx: double, features: vector, label: double, rawPrediction: vector, probability: vector, prediction: double, article_class: string]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv8sBpuJymgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6feeec80-a0be-4311-f87d-b9075205177e"
      },
      "source": [
        "pred.select(\"review\",\"polarity\", \"label\", \"prediction\",\"article_class\").show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------+-----+----------+-------------+\n",
            "|              review|polarity|label|prediction|article_class|\n",
            "+--------------------+--------+-----+----------+-------------+\n",
            "| My wife and I's ...|positive|  0.0|       0.0|         fake|\n",
            "|A bunch of us got...|positive|  1.0|       0.0|         real|\n",
            "|A lovely hotel in...|negative|  1.0|       1.0|         real|\n",
            "|A recent stay at ...|negative|  0.0|       0.0|         fake|\n",
            "|Affinia hotel in ...|negative|  0.0|       0.0|         fake|\n",
            "|After considering...|negative|  0.0|       0.0|         fake|\n",
            "|After reading so ...|positive|  1.0|       1.0|         real|\n",
            "|After reading the...|negative|  1.0|       1.0|         real|\n",
            "|After reading the...|positive|  1.0|       1.0|         real|\n",
            "|After some delibe...|positive|  1.0|       1.0|         real|\n",
            "|All I can say is ...|negative|  1.0|       0.0|         real|\n",
            "|Amalifa Hotel in ...|positive|  0.0|       0.0|         fake|\n",
            "|Awesome hotel, ou...|positive|  1.0|       1.0|         real|\n",
            "|Booked a room onl...|positive|  1.0|       1.0|         real|\n",
            "|Chicago has many ...|negative|  0.0|       0.0|         fake|\n",
            "|Conrad Chicago is...|positive|  0.0|       0.0|         fake|\n",
            "|Couldn't have ask...|positive|  0.0|       0.0|         fake|\n",
            "|DO NOT STAY HERE!...|negative|  0.0|       0.0|         fake|\n",
            "|Don't ever stay a...|negative|  0.0|       0.0|         fake|\n",
            "|Downtown Chicago ...|positive|  0.0|       0.0|         fake|\n",
            "+--------------------+--------+-----+----------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bFAWwH5ymgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "238bbbe4-843f-43db-a395-f78b0d05489f"
      },
      "source": [
        "pred.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+---------+--------+---------------+-------------------+------------------+---------------+-------------------+-----------------+---------+-------------------+-----------------------+----------------------+--------------------+-----------------------+---------------------+----------------------+--------------------------+-------------------------+----------------------+--------------------------+------------------------+--------------------+--------------------+------------+--------------------+-----+--------------------+--------------------+----------+-------------+\n",
            "|              Review|             stemmed|sentiment|polarity|length_in_words|average_word_length|capital_char_ratio|long_word_ratio|non_stop_word_ratio|punctuation_ratio|real_fake|length_in_words_vec|average_word_length_vec|capital_char_ratio_vec| long_word_ratio_vec|non_stop_word_ratio_vec|punctuation_ratio_vec|length_in_words_scaled|average_word_length_scaled|capital_char_ratio_scaled|long_word_ratio_scaled|non_stop_word_ratio_scaled|punctuation_ratio_scaled|         rawFeatures|              TF_IDF|polarity_idx|            features|label|       rawPrediction|         probability|prediction|article_class|\n",
            "+--------------------+--------------------+---------+--------+---------------+-------------------+------------------+---------------+-------------------+-----------------+---------+-------------------+-----------------------+----------------------+--------------------+-----------------------+---------------------+----------------------+--------------------------+-------------------------+----------------------+--------------------------+------------------------+--------------------+--------------------+------------+--------------------+-----+--------------------+--------------------+----------+-------------+\n",
            "| My wife and I's ...|[wife, stay, sher...|   0.9744|positive|            248|           4.278226|       0.030541012|     0.23790322|         0.53629035|       0.12903225|     fake|            [248.0]|    [4.278225898742676]|  [0.03054101206362...|[0.23790322244167...|   [0.5362903475761414]| [0.12903225421905...|  [2.8803515145980305]|      [16.049619309523358]|     [1.0284257078117143]|   [4.961675448900411]|      [11.042779376586633]|     [2.676099206262784]|(20000,[1,76,494,...|(20000,[1,76,494,...|         1.0|(20008,[1,76,494,...|  0.0|[2.63992114503762...|[0.93338706173354...|       0.0|         fake|\n",
            "|A bunch of us got...|[bunch, got, toge...|   0.9714|positive|             65|          4.0615387|        0.00754717|     0.13846155|         0.47692308|      0.015384615|     real|             [65.0]|   [4.0615386962890625]|  [0.00754716992378...|[0.13846154510974...|   [0.4769230782985687]| [0.01538461539894...|  [0.7549308405196451]|      [15.236724621178785]|     [0.25414035247660...|  [2.8877341043847764]|       [9.820345186253633]|    [0.3190733767068506]|(20000,[399,494,1...|(20000,[399,494,1...|         1.0|(20008,[399,494,1...|  1.0|[0.39400744724328...|[0.59724703725796...|       0.0|         real|\n",
            "|A lovely hotel in...|[love, hotel, tre...|  -0.2756|negative|            128|          4.6640625|         0.0109375|      0.3046875|           0.578125|        0.1484375|     real|            [128.0]|            [4.6640625]|  [0.01093749981373...|         [0.3046875]|             [0.578125]|          [0.1484375]|  [1.4866330397925318]|      [17.497072228659846]|     [0.3683049521803317]|  [6.3545187527145925]|      [11.904198641544161]|    [3.0785595302028717]|(20000,[93,103,49...|(20000,[93,103,49...|         0.0|(20008,[93,103,49...|  1.0|[-1.4078081075094...|[0.19658000555414...|       1.0|         real|\n",
            "|A recent stay at ...|[recent, stay, ja...|  -0.4512|negative|            266|          4.2593985|       0.020833334|     0.22180451|         0.53759396|       0.13157895|     fake|            [266.0]|    [4.259398460388184]|  [0.02083333395421...|[0.22180451452732...|   [0.5375939607620239]| [0.1315789520740509]|   [3.089409285818855]|      [15.978988813304838]|     [0.7015332751026112]|   [4.625923108104589]|      [11.069622136052981]|    [2.7289171326765564]|(20000,[76,390,49...|(20000,[76,390,49...|         0.0|(20008,[76,390,49...|  0.0|[1.09756190078510...|[0.74980300055859...|       0.0|         fake|\n",
            "|Affinia hotel in ...|[affinia, hotel, ...|  -0.2479|negative|            145|           4.303448|       0.026687598|      0.2137931|         0.56551725|      0.089655176|     fake|            [145.0]|     [4.30344820022583]|  [0.02668759785592...|[0.2137930989265442]|    [0.565517246723175]| [0.08965517580509...|  [1.6840764903899774]|      [16.144239917807184]|      [0.898667393785091]|   [4.458838174620524]|      [11.644591810096111]|    [1.8594276776877774]|(20000,[1,192,494...|(20000,[1,192,494...|         0.0|(20008,[1,192,494...|  0.0|[1.61684946206694...|[0.83436017370022...|       0.0|         fake|\n",
            "|After considering...|[after, consid, s...|  -0.7778|negative|             81|           4.716049|        0.02284264|     0.32098764|          0.6296296|       0.14814815|     fake|             [81.0]|   [4.7160491943359375]|  [0.02284264005720...|[0.32098764181137...|   [0.6296296119689941]| [0.14814814925193...|  [0.9407599704937115]|      [17.692098548681322]|     [0.7691938374596899]|   [6.694472169944585]|      [12.964732491203938]|    [3.0725584623930664]|(20000,[1,512,179...|(20000,[1,512,179...|         0.0|(20008,[1,512,179...|  0.0|[0.50892532912891...|[0.62455451275886...|       0.0|         fake|\n",
            "|After reading so ...|[after, read, man...|   0.9667|positive|            114|           4.359649|       0.036053132|     0.27192983|         0.57894737|        0.1491228|     real|            [114.0]|    [4.359649181365967]|  [0.0360531322658062]|[0.2719298303127289]|   [0.5789473652839661]| [0.14912280440330...|  [1.3240325510652236]|      [16.355075991794298]|     [1.2140386177134848]|   [5.671329497090405]|      [11.921132003180903]|     [3.092772585541885]|(20000,[1,47,158,...|(20000,[1,47,158,...|         1.0|(20008,[1,47,158,...|  1.0|[-2.5088902224539...|[0.07523728768384...|       1.0|         real|\n",
            "|After reading the...|[after, read, tri...|   0.8778|negative|            124|            4.41129|       0.033043478|     0.29032257|          0.5483871|       0.19354838|     real|            [124.0]|    [4.411290168762207]|  [0.03304347768425...|[0.29032257199287...|   [0.5483871102333069]| [0.19354838132858...|  [1.4401757572990153]|       [16.54880540396048]|     [1.1126927246288634]|   [6.054925876726322]|      [11.291864376526979]|     [4.014148809394176]|(20000,[344,504,7...|(20000,[344,504,7...|         0.0|(20008,[344,504,7...|  1.0|[-0.3934362074744...|[0.40289037810747...|       1.0|         real|\n",
            "|After reading the...|[after, read, rev...|   0.9253|positive|             75|               4.04|        0.03215434|     0.18666667|         0.53333336|       0.10666667|     real|             [75.0]|    [4.039999961853027]|  [0.03215434029698...|[0.18666666746139...|   [0.5333333611488342]| [0.1066666692495346]|  [0.8710740467534366]|      [15.155922789697918]|     [1.0827522712821969]|  [3.8930932148190363]|      [10.981891932156772]|    [2.2122421300086055]|(20000,[344,419,1...|(20000,[344,419,1...|         1.0|(20008,[344,419,1...|  1.0|[-0.4602934933825...|[0.38691620138218...|       1.0|         real|\n",
            "|After some delibe...|[after, deliber, ...|    0.996|positive|            178|           4.247191|        0.04336735|     0.20786516|          0.5898876|       0.10674157|     real|            [178.0]|    [4.247190952301025]|  [0.04336734861135...|[0.2078651636838913]|   [0.5898876190185547]| [0.10674156993627...|  [2.0673490709614897]|      [15.933192760886381]|     [1.4603345854629786]|   [4.335206008337643]|       [12.14640327435139]|    [2.2137955529845033]|(20000,[1,76,494,...|(20000,[1,76,494,...|         1.0|(20008,[1,76,494,...|  1.0|[-0.6287273516453...|[0.34779916293102...|       1.0|         real|\n",
            "|All I can say is ...|[all, say, avoid,...|  -0.9098|negative|             80|              4.025|       0.063253015|         0.2125|                0.6|            0.125|     real|             [80.0]|    [4.025000095367432]|  [0.0632530152797699]|[0.21250000596046...|   [0.6000000238418579]|              [0.125]|  [0.9291456498703323]|       [15.09965129948553]|      [2.129956495050309]|   [4.431869613383333]|      [12.354628270261118]|     [2.592471183328734]|(20000,[494,1882,...|(20000,[494,1882,...|         0.0|(20008,[494,1882,...|  1.0|[0.37724328337122...|[0.59320804471983...|       0.0|         real|\n",
            "|Amalifa Hotel in ...|[amalifa, hotel, ...|   0.8764|positive|             37|           5.027027|       0.026455026|      0.3783784|          0.5945946|       0.08108108|     fake|             [37.0]|    [5.027027130126953]|  [0.02645502611994...|[0.37837839126586...|   [0.5945945978164673]| [0.0810810774564743]|  [0.42972986306502...|       [18.85872172409009]|      [0.890835867059911]|  [7.8914054003559215]|       [12.24332489270709]|    [1.6816028545532375]|(20000,[504,2067,...|(20000,[504,2067,...|         1.0|(20008,[504,4418,...|  0.0|[0.68908302432146...|[0.66576290991175...|       0.0|         fake|\n",
            "|Awesome hotel, ou...|[awesom, hotel, r...|   0.9052|positive|             51|           4.352941|       0.030172413|     0.25490198|          0.5686275|       0.19607843|     real|             [51.0]|    [4.352941036224365]|  [0.03017241321504...|[0.2549019753932953]|   [0.5686274766921997]| [0.19607843458652...|  [0.5923303517923368]|      [16.329910612886334]|     [1.0160136590242956]|   [5.316198999764311]|      [11.708634699388485]|     [4.066621530702193]|(20000,[494,525,2...|(20000,[494,525,2...|         1.0|(20008,[494,525,2...|  1.0|[-1.5056089974837...|[0.18159045141442...|       1.0|         real|\n",
            "|Booked a room onl...|[book, room, onli...|   0.9926|positive|            137|           4.072993|       0.013840831|     0.18978103|          0.5182482|       0.13868614|     real|            [137.0]|     [4.07299280166626]|  [0.01384083088487...|[0.18978102505207...|   [0.5182482004165649]| [0.1386861354112625]|   [1.591161925402944]|      [15.279694308891912]|     [0.46607055030874...|  [3.9580458095681004]|       [10.67127268909246]|    [2.8763184766473984]|(20000,[93,494,76...|(20000,[93,494,76...|         1.0|(20008,[93,494,76...|  1.0|[-1.3731768446419...|[0.20210706425817...|       1.0|         real|\n",
            "|Chicago has many ...|[chicago, mani, w...|  -0.4049|negative|            257|           4.365759|       0.030927835|     0.26848248|          0.6108949|       0.15564202|     fake|            [257.0]|    [4.365758895874023]|  [0.03092783503234...|[0.2684824764728546]|   [0.6108949184417725]| [0.15564201772212...|   [2.984880400208443]|       [16.37799637848379]|      [1.041451428261855]|  [5.5994319803799275]|      [12.578965549389455]|    [3.2279795668780937]|(20000,[1,68,93,1...|(20000,[1,68,93,1...|         0.0|(20008,[1,68,93,1...|  0.0|[2.80411312949842...|[0.94289768559056...|       0.0|         fake|\n",
            "|Conrad Chicago is...|[conrad, chicago,...|   0.6808|positive|             38|          4.4210525|       0.029069768|     0.28947368|         0.55263156|       0.10526316|     fake|             [38.0]|      [4.4210524559021]|  [0.02906976826488...|[0.28947368264198...|   [0.5526315569877625]| [0.10526315867900...|  [0.4413441836884079]|       [16.58542829295556]|     [0.9788836382194119]|   [6.037221562308377]|      [11.379261976161136]|    [2.1831336443319156]|(20000,[494,1067,...|(20000,[494,1067,...|         1.0|(20008,[494,1067,...|  0.0|[0.58256383938514...|[0.64165713213205...|       0.0|         fake|\n",
            "|Couldn't have ask...|[could, ask, bett...|   0.9736|positive|            116|          4.5603447|       0.021778584|      0.2672414|          0.5862069|       0.14655173|     fake|            [116.0]|    [4.560344696044922]|  [0.02177858352661...|[0.26724138855934...|   [0.5862069129943848]| [0.1465517282485962]|   [1.347261192311982]|      [17.107978406010606]|     [0.7333632275130049]|   [5.573548029052134]|      [12.070613686195115]|     [3.039449058812074]|(20000,[76,103,49...|(20000,[76,103,49...|         1.0|(20008,[76,103,49...|  0.0|[0.80203854180483...|[0.69041037602605...|       0.0|         fake|\n",
            "|DO NOT STAY HERE!...|[not, stay, here,...|   -0.742|negative|             99|          4.1616163|        0.06398104|     0.22222222|          0.5858586|        0.1010101|     fake|             [99.0]|    [4.161616325378418]|  [0.06398104131221...|[0.2222222238779068]|   [0.5858585834503174]| [0.10101009905338...|  [1.1498177417145363]|      [15.612162451321389]|     [2.1544717496910453]|   [4.634634794345351]|      [12.063441216425954]|    [2.0949261681686093]|(20000,[93,360,49...|(20000,[93,360,49...|         0.0|(20008,[93,360,49...|  0.0|[1.62403920469460...|[0.83535143245233...|       0.0|         fake|\n",
            "|Don't ever stay a...|[ever, stay, hote...|  -0.8614|negative|             99|           4.010101|       0.049411766|     0.18181819|          0.5555556|        0.2020202|     fake|             [99.0]|    [4.010100841522217]|  [0.04941176623106...|[0.1818181872367859]|   [0.5555555820465088]| [0.20202019810676...|  [1.1498177417145363]|       [15.04375725417999]|     [1.6638718636614014]|   [3.791974007403418]|      [11.439470711524887]|     [4.189852336337219]|(20000,[76,494,94...|(20000,[76,494,94...|         0.0|(20008,[76,494,94...|  0.0|[0.65029753630074...|[0.65707750866624...|       0.0|         fake|\n",
            "|Downtown Chicago ...|[downtown, chicag...|   0.9702|positive|            118|          4.8728814|       0.020618556|     0.38135594|         0.60169494|      0.059322033|     fake|            [118.0]|   [4.8728814125061035]|  [0.02061855606734...|[0.3813559412956238]|   [0.6016949415206909]| [0.05932203307747...|  [1.3704898335587403]|       [18.28044929421793]|     [0.6943009312672499]|  [7.9535047562573675]|       [12.38952839865877]|     [1.230325290318694]|(20000,[103,1041,...|(20000,[103,1041,...|         1.0|(20008,[103,1041,...|  0.0|[2.22632826395142...|[0.90258900916717...|       0.0|         fake|\n",
            "+--------------------+--------------------+---------+--------+---------------+-------------------+------------------+---------------+-------------------+-----------------+---------+-------------------+-----------------------+----------------------+--------------------+-----------------------+---------------------+----------------------+--------------------------+-------------------------+----------------------+--------------------------+------------------------+--------------------+--------------------+------------+--------------------+-----+--------------------+--------------------+----------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGux6YdQymgv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a0cd201-e004-46b3-d122-fee72bc1ebdc"
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "eval = MulticlassClassificationEvaluator(labelCol=\"label\",predictionCol=\"prediction\",metricName=\"accuracy\")\n",
        "acc_our_data = eval.evaluate(pred)\n",
        "print(\"our data: \", acc_our_data)\n",
        "acc_original_data = eval.evaluate(pred_original)\n",
        "print(\"original data: \", acc_original_data)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "our data:  0.8621700879765396\n",
            "original data:  0.609\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-NRE3_Xymgv"
      },
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(hashingTF.numFeatures, [10000,20000,50000]) \\\n",
        "    .addGrid(lr.regParam, [0.1, 0.3 ,0.5]) \\\n",
        "    .build()\n",
        "crossval = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid,evaluator= MulticlassClassificationEvaluator(),numFolds=4,parallelism = 100 )  # use 3+ folds in practice\n",
        "\n",
        "### ON REFINEMENT DO 5 FOLD? MAYBE JUST FOR BEST PRELIMINARY MODEL?###"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwNf3KNLymgv"
      },
      "source": [
        "cvModel = crossval.fit(train_split)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IAzifUZymgv"
      },
      "source": [
        "p = cvModel.transform(test_split)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2csZqvzKymgv"
      },
      "source": [
        "acc = eval.evaluate(p)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMwgXgVmymgv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65c99035-a4a0-416a-c190-afe35d4f16d5"
      },
      "source": [
        "acc"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8709677419354839"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esqtuOF6ymgv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "c6c3407f-f60a-4da1-e585-ee5fd4693c80"
      },
      "source": [
        "params = [{\n",
        "      p.name: v\n",
        "      for p,\n",
        "      v in m.items()\n",
        "   }\n",
        "   for m in cvModel.getEstimatorParamMaps()\n",
        "]\n",
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame.from_dict([{\n",
        "      cvModel.getEvaluator().getMetricName(): metric,\n",
        "      ** ps\n",
        "   }\n",
        "   for ps, metric in zip(params, cvModel.avgMetrics)\n",
        "])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         f1  numFeatures  regParam\n",
              "0  0.852439        10000       0.1\n",
              "1  0.856338        10000       0.3\n",
              "2  0.857086        10000       0.5\n",
              "3  0.851598        20000       0.1\n",
              "4  0.857179        20000       0.3\n",
              "5  0.857832        20000       0.5\n",
              "6  0.853992        50000       0.1\n",
              "7  0.856405        50000       0.3\n",
              "8  0.858652        50000       0.5"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>numFeatures</th>\n",
              "      <th>regParam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.852439</td>\n",
              "      <td>10000</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.856338</td>\n",
              "      <td>10000</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.857086</td>\n",
              "      <td>10000</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.851598</td>\n",
              "      <td>20000</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.857179</td>\n",
              "      <td>20000</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.857832</td>\n",
              "      <td>20000</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.853992</td>\n",
              "      <td>50000</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.856405</td>\n",
              "      <td>50000</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.858652</td>\n",
              "      <td>50000</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vqnv3JLV2nsy"
      },
      "source": [
        "# SVC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HZrDboiymgw"
      },
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, IndexToString, StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "columns_to_be_scaled = ['length_in_words', 'average_word_length', 'capital_char_ratio', 'long_word_ratio', 'non_stop_word_ratio', 'punctuation_ratio']\n",
        "assemblers = [VectorAssembler(inputCols=[col], outputCol=col + \"_vec\") for col in columns_to_be_scaled]\n",
        "scalers  = [StandardScaler(inputCol=col + \"_vec\", outputCol=col + \"_scaled\") for col in columns_to_be_scaled]\n",
        "scaling_pipeline = Pipeline(stages=assemblers + scalers)\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"stemmed\", outputCol=\"rawFeatures\")\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"TF_IDF\", minDocFreq=2)\n",
        "label_strIdx1 = StringIndexer(inputCol=\"polarity\", outputCol=\"polarity_idx\")\n",
        "assembles = VectorAssembler(inputCols = ['TF_IDF','sentiment','polarity_idx'],outputCol=\"features\")\n",
        "label_strIdx2 = StringIndexer(inputCol=\"real_fake\", outputCol=\"label\")\n",
        "svc = LinearSVC(featuresCol=\"features\",labelCol=\"label\")\n",
        "label_idxStr = IndexToString(inputCol = \"label\", outputCol = \"article_class\")\n",
        "\n",
        "pipeline_svc = Pipeline(stages=[scaling_pipeline, hashingTF, idf,label_strIdx1 , assembles, label_strIdx2, svc ,label_idxStr])"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn-VtpOkymgw"
      },
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(hashingTF.numFeatures, [10000,20000,50000]) \\\n",
        "    .addGrid(svc.regParam, [0.0001, 0.001, 0.01, 0.1, 1]) \\\n",
        "    .build()\n",
        "crossval_svc = CrossValidator(estimator=pipeline_svc, estimatorParamMaps=paramGrid,evaluator= MulticlassClassificationEvaluator(),numFolds=2,parallelism = 100 )  # use 3+ folds in practice"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqS5YpWpymgw"
      },
      "source": [
        "cvModel_svc = crossval_svc.fit(combined_train)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af7bBGIrymgw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f08a917-6de3-48b9-a98f-1bdfa5b2c630"
      },
      "source": [
        "pred_old_svc = cvModel_svc.transform(old_test_split)\n",
        "pred_new_svc = cvModel_svc.transform(new_test_split)\n",
        "pred_combined_svc = cvModel_svc.transform(combined_test)\n",
        "\n",
        "pred_old_svc.cache()\n",
        "pred_new_svc.cache()\n",
        "pred_combined_svc.cache()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Review: string, stemmed: array<string>, sentiment: float, polarity: string, length_in_words: int, average_word_length: float, capital_char_ratio: float, long_word_ratio: float, non_stop_word_ratio: float, punctuation_ratio: float, real_fake: string, length_in_words_vec: vector, average_word_length_vec: vector, capital_char_ratio_vec: vector, long_word_ratio_vec: vector, non_stop_word_ratio_vec: vector, punctuation_ratio_vec: vector, length_in_words_scaled: vector, average_word_length_scaled: vector, capital_char_ratio_scaled: vector, long_word_ratio_scaled: vector, non_stop_word_ratio_scaled: vector, punctuation_ratio_scaled: vector, rawFeatures: vector, TF_IDF: vector, polarity_idx: double, features: vector, label: double, rawPrediction: vector, prediction: double, article_class: string]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFJbAweqymgw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20417cf1-47f5-4dc7-d95c-c0f04f42bc49"
      },
      "source": [
        "eval = MulticlassClassificationEvaluator(labelCol=\"label\",predictionCol=\"prediction\",metricName=\"accuracy\")\n",
        "acc_new_data = eval.evaluate(pred_new_svc)\n",
        "print(\"new data: \", acc_new_data)\n",
        "acc_old_data = eval.evaluate(pred_old_svc)\n",
        "print(\"old data: \", acc_old_data)\n",
        "acc_combined_data = eval.evaluate(pred_combined_svc)\n",
        "print(\"combined data: \", acc_combined_data)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "our data:  0.844574780058651\n",
            "original data:  0.635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ-u8rPzymgw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "632ad18d-e7f2-4dc1-d578-dbea91b4a22c"
      },
      "source": [
        "params = [{\n",
        "      p.name: v\n",
        "      for p,\n",
        "      v in m.items()\n",
        "   }\n",
        "   for m in cvModel_svc.getEstimatorParamMaps()\n",
        "]\n",
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame.from_dict([{\n",
        "      cvModel_svc.getEvaluator().getMetricName(): metric,\n",
        "      ** ps\n",
        "   }\n",
        "   for ps, metric in zip(params, cvModel_svc.avgMetrics)\n",
        "])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          f1  numFeatures  regParam\n",
              "0   0.822145        10000    0.0001\n",
              "1   0.822926        10000    0.0010\n",
              "2   0.819659        10000    0.0100\n",
              "3   0.817264        10000    0.1000\n",
              "4   0.795731        10000    1.0000\n",
              "5   0.816540        20000    0.0001\n",
              "6   0.815763        20000    0.0010\n",
              "7   0.813276        20000    0.0100\n",
              "8   0.815647        20000    0.1000\n",
              "9   0.806290        20000    1.0000\n",
              "10  0.815737        50000    0.0001\n",
              "11  0.820492        50000    0.0010\n",
              "12  0.815858        50000    0.0100\n",
              "13  0.812628        50000    0.1000\n",
              "14  0.803083        50000    1.0000"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>numFeatures</th>\n",
              "      <th>regParam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.822145</td>\n",
              "      <td>10000</td>\n",
              "      <td>0.0001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.822926</td>\n",
              "      <td>10000</td>\n",
              "      <td>0.0010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.819659</td>\n",
              "      <td>10000</td>\n",
              "      <td>0.0100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.817264</td>\n",
              "      <td>10000</td>\n",
              "      <td>0.1000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.795731</td>\n",
              "      <td>10000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.816540</td>\n",
              "      <td>20000</td>\n",
              "      <td>0.0001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.815763</td>\n",
              "      <td>20000</td>\n",
              "      <td>0.0010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.813276</td>\n",
              "      <td>20000</td>\n",
              "      <td>0.0100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.815647</td>\n",
              "      <td>20000</td>\n",
              "      <td>0.1000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.806290</td>\n",
              "      <td>20000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.815737</td>\n",
              "      <td>50000</td>\n",
              "      <td>0.0001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.820492</td>\n",
              "      <td>50000</td>\n",
              "      <td>0.0010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.815858</td>\n",
              "      <td>50000</td>\n",
              "      <td>0.0100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.812628</td>\n",
              "      <td>50000</td>\n",
              "      <td>0.1000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.803083</td>\n",
              "      <td>50000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AziTYK3ymgx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6848463f-a86e-49a1-a2d8-ef410b1d66b9"
      },
      "source": [
        "svc.params"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Param(parent='LinearSVC_b617b43c3ecc', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'),\n",
              " Param(parent='LinearSVC_b617b43c3ecc', name='featuresCol', doc='features column name.'),\n",
              " Param(parent='LinearSVC_b617b43c3ecc', name='fitIntercept', doc='whether to fit an intercept term.'),\n",
              " Param(parent='LinearSVC_b617b43c3ecc', name='labelCol', doc='label column name.'),\n",
              " Param(parent='LinearSVC_b617b43c3ecc', name='maxIter', doc='max number of iterations (>= 0).'),\n",
              " Param(parent='LinearSVC_b617b43c3ecc', name='predictionCol', doc='prediction column name.'),\n",
              " Param(parent='LinearSVC_b617b43c3ecc', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'),\n",
              " Param(parent='LinearSVC_b617b43c3ecc', name='regParam', doc='regularization parameter (>= 0).'),\n",
              " Param(parent='LinearSVC_b617b43c3ecc', name='standardization', doc='whether to standardize the training features before fitting the model.'),\n",
              " Param(parent='LinearSVC_b617b43c3ecc', name='threshold', doc='The threshold in binary classification applied to the linear model prediction.  This threshold can be any real number, where Inf will make all predictions 0.0 and -Inf will make all predictions 1.0.'),\n",
              " Param(parent='LinearSVC_b617b43c3ecc', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'),\n",
              " Param(parent='LinearSVC_b617b43c3ecc', name='weightCol', doc='weight column name. If this is not set or empty, we treat all instance weights as 1.0.')]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IavGTjXr21fD"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTYWEIrvymgx"
      },
      "source": [
        "#greg\n",
        "\n",
        "from pyspark.ml.feature import HashingTF, IDF, IndexToString, StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "columns_to_be_scaled = ['length_in_words', 'average_word_length', 'capital_char_ratio', 'long_word_ratio', 'non_stop_word_ratio', 'punctuation_ratio']\n",
        "assemblers = [VectorAssembler(inputCols=[col], outputCol=col + \"_vec\") for col in columns_to_be_scaled]\n",
        "scalers  = [StandardScaler(inputCol=col + \"_vec\", outputCol=col + \"_scaled\") for col in columns_to_be_scaled]\n",
        "scaling_pipeline = Pipeline(stages=assemblers + scalers)\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"stemmed\", outputCol=\"rawFeatures\")\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"TF_IDF\", minDocFreq=2)\n",
        "label_strIdx1 = StringIndexer(inputCol=\"polarity\", outputCol=\"polarity_idx\")\n",
        "assembles = VectorAssembler(inputCols = ['TF_IDF','sentiment','polarity_idx'],outputCol=\"features\")\n",
        "label_strIdx2 = StringIndexer(inputCol=\"real_fake\", outputCol=\"label\")\n",
        "rfc = RandomForestClassifier(featuresCol=\"features\",labelCol=\"label\")\n",
        "label_idxStr = IndexToString(inputCol = \"label\", outputCol = \"article_class\")\n",
        "\n",
        "pipeline_rfc = Pipeline(stages=[scaling_pipeline, hashingTF, idf,label_strIdx1 , assembles, label_strIdx2, rfc ,label_idxStr])"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYVbeEqdymgx"
      },
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(hashingTF.numFeatures, [10000,20000,50000]) \\\n",
        "    .addGrid(rfc.maxDepth, [3, 5, 9]) \\\n",
        "    .addGrid(rfc.numTrees, [10, 20, 50, 100]) \\\n",
        "    .build()\n",
        "crossval_rfc = CrossValidator(estimator=pipeline_rfc, estimatorParamMaps=paramGrid,evaluator= MulticlassClassificationEvaluator(),numFolds=2,parallelism = 100 )  # use 3+ folds in practice"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OckAS0jMymgx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "outputId": "7e301e84-0e12-4be0-ce1f-56496eb5c4e3"
      },
      "source": [
        "cvModel_rfc = crossval_rfc.fit(train_split)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/global/software/jupyterhub-spark/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-5b9e3391577b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcvModel_rfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrossval_rfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/global/software/jupyterhub-spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/global/software/jupyterhub-spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parallelFitTasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnFolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/global/software/jupyterhub-spark/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    735\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/global/software/jupyterhub-spark/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sI-0Qbn3_Jnh"
      },
      "source": [
        "pred_rfc = cvModel_rfc.transform(test_split)\n",
        "pred_original_rfc = cvModel_rfc.transform(df_test)\n",
        "pred_rfc.cache()\n",
        "pred_original_rfc.cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0TjecAq_m5R"
      },
      "source": [
        "eval = MulticlassClassificationEvaluator(labelCol=\"label\",predictionCol=\"prediction\",metricName=\"accuracy\")\n",
        "acc_our_data_rfc = eval.evaluate(pred_rfc)\n",
        "print(\"our data: \", acc_our_data_rfc)\n",
        "acc_original_data_rfc = eval.evaluate(pred_original_rfc)\n",
        "print(\"original data: \", acc_original_data_rfc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VylJyR2Y56yh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2NXdB_0_w_R"
      },
      "source": [
        "params = [{\n",
        "      p.name: v\n",
        "      for p,\n",
        "      v in m.items()\n",
        "   }\n",
        "   for m in cvModel_rfc.getEstimatorParamMaps()\n",
        "]\n",
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame.from_dict([{\n",
        "      cvModel_rfc.getEvaluator().getMetricName(): metric,\n",
        "      ** ps\n",
        "   } for ps, metric in zip(params, cvModel_rfc.avgMetrics)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2FXBLjS27SR"
      },
      "source": [
        "# Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0NiNweqAN8W"
      },
      "source": [
        "#greg\n",
        "\n",
        "from pyspark.ml.feature import HashingTF, IDF, IndexToString, StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "columns_to_be_scaled = ['length_in_words', 'average_word_length', 'capital_char_ratio', 'long_word_ratio', 'non_stop_word_ratio', 'punctuation_ratio']\n",
        "assemblers = [VectorAssembler(inputCols=[col], outputCol=col + \"_vec\") for col in columns_to_be_scaled]\n",
        "scalers  = [StandardScaler(inputCol=col + \"_vec\", outputCol=col + \"_scaled\") for col in columns_to_be_scaled]\n",
        "scaling_pipeline = Pipeline(stages=assemblers + scalers)\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"stemmed\", outputCol=\"rawFeatures\")\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"TF_IDF\", minDocFreq=2)\n",
        "label_strIdx1 = StringIndexer(inputCol=\"polarity\", outputCol=\"polarity_idx\")\n",
        "assembles = VectorAssembler(inputCols = ['TF_IDF','sentiment','polarity_idx'],outputCol=\"features\")\n",
        "label_strIdx2 = StringIndexer(inputCol=\"real_fake\", outputCol=\"label\")\n",
        "gbtc = GBTClassifier(featuresCol=\"features\",labelCol=\"label\")\n",
        "label_idxStr = IndexToString(inputCol = \"label\", outputCol = \"article_class\")\n",
        "\n",
        "pipeline_gbtc = Pipeline(stages=[scaling_pipeline, hashingTF, idf,label_strIdx1 , assembles, label_strIdx2, gbtc ,label_idxStr])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aEGVcyLBy51"
      },
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(hashingTF.numFeatures, [10000,16384,32768]) \\\n",
        "    .addGrid(gbtc.maxDepth, [3, 5, 7]) \\\n",
        "    .addGrid(gbtc.stepSize, [0.1, 0.5, 1]) \\\n",
        "    .build()\n",
        "crossval_gbtc = CrossValidator(estimator=pipeline_gbtc, estimatorParamMaps=paramGrid,evaluator= MulticlassClassificationEvaluator(),numFolds=2,parallelism = 100 )  # use 3+ folds in practice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eb-J_DdCq1I"
      },
      "source": [
        "cvModel_gbtc = crossval_gbtc.fit(train_split)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdH_7sRnC6Dg"
      },
      "source": [
        "pred_gbtc = cvModel_gbtc.transform(test_split)\n",
        "pred_original_gbtc = cvModel_gbtc.transform(df_test)\n",
        "pred_gbtc.cache()\n",
        "pred_original_gbtc.cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_91JR4vDC-xA"
      },
      "source": [
        "eval = MulticlassClassificationEvaluator(labelCol=\"label\",predictionCol=\"prediction\",metricName=\"accuracy\")\n",
        "acc_our_data_gbtc = eval.evaluate(pred_gbtc)\n",
        "print(\"our data: \", acc_our_data_gbtc)\n",
        "acc_original_data_gbtc = eval.evaluate(pred_original_gbtc)\n",
        "print(\"original data: \", acc_original_data_gbtc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsxqI7FYDKtv"
      },
      "source": [
        "params = [{\n",
        "      p.name: v\n",
        "      for p,\n",
        "      v in m.items()\n",
        "   }\n",
        "   for m in cvModel_gbtc.getEstimatorParamMaps()\n",
        "]\n",
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame.from_dict([{\n",
        "      cvModel_gbtc.getEvaluator().getMetricName(): metric,\n",
        "      ** ps\n",
        "   }\n",
        "   for ps, metric in zip(params, cvModel_gbtc.avgMetrics)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DaBrnnNDeJe"
      },
      "source": [
        "#greg\n",
        "\n",
        "from pyspark.ml.feature import HashingTF, IDF, IndexToString, StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "columns_to_be_scaled = ['length_in_words', 'average_word_length', 'capital_char_ratio', 'long_word_ratio', 'non_stop_word_ratio', 'punctuation_ratio']\n",
        "assemblers = [VectorAssembler(inputCols=[col], outputCol=col + \"_vec\") for col in columns_to_be_scaled]\n",
        "scalers  = [StandardScaler(inputCol=col + \"_vec\", outputCol=col + \"_scaled\") for col in columns_to_be_scaled]\n",
        "scaling_pipeline = Pipeline(stages=assemblers + scalers)\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"stemmed\", outputCol=\"rawFeatures\")\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"TF_IDF\", minDocFreq=2)\n",
        "label_strIdx1 = StringIndexer(inputCol=\"polarity\", outputCol=\"polarity_idx\")\n",
        "assembles = VectorAssembler(inputCols = ['TF_IDF','sentiment','polarity_idx'],outputCol=\"features\")\n",
        "label_strIdx2 = StringIndexer(inputCol=\"real_fake\", outputCol=\"label\")\n",
        "nb = NaiveBayes(featuresCol=\"features\",labelCol=\"label\", modelType=\"gaussian\")\n",
        "label_idxStr = IndexToString(inputCol = \"label\", outputCol = \"article_class\")\n",
        "\n",
        "pipeline_nb = Pipeline(stages=[scaling_pipeline, hashingTF, idf,label_strIdx1 , assembles, label_strIdx2, nb ,label_idxStr])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Wisnv1GSW88"
      },
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(hashingTF.numFeatures, [10000,16384,32768]) \\\n",
        "    .addGrid(nb.smoothing, [0.1, 1, 10]) \\\n",
        "    .build()\n",
        "crossval_nb = CrossValidator(estimator=pipeline_nb, estimatorParamMaps=paramGrid,evaluator= MulticlassClassificationEvaluator(),numFolds=2,parallelism = 100 )  # use 3+ folds in practice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEwcLgl6TJwc"
      },
      "source": [
        "cvModel_nb = crossval_nb.fit(train_split)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1iRkcPjTU5C"
      },
      "source": [
        "pred_nb = cvModel_nb.transform(test_split)\n",
        "pred_original_nb = cvModel_nb.transform(df_test)\n",
        "pred_nb.cache()\n",
        "pred_original_nb.cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCG5oi5XTgCW"
      },
      "source": [
        "eval = MulticlassClassificationEvaluator(labelCol=\"label\",predictionCol=\"prediction\",metricName=\"accuracy\")\n",
        "acc_our_data_nb = eval.evaluate(pred_gbtc)\n",
        "print(\"our data: \", acc_our_data_nb)\n",
        "acc_original_data_nb = eval.evaluate(pred_original_nb)\n",
        "print(\"original data: \", acc_original_data_nb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-cMf5CwT2ge"
      },
      "source": [
        "params = [{\n",
        "      p.name: v\n",
        "      for p,\n",
        "      v in m.items()\n",
        "   }\n",
        "   for m in cvModel_nb.getEstimatorParamMaps()\n",
        "]\n",
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame.from_dict([{\n",
        "      cvModel_nb.getEvaluator().getMetricName(): metric,\n",
        "      ** ps\n",
        "   }\n",
        "   for ps, metric in zip(params, cvModel_nb.avgMetrics)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTsx2y3IV4aG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}