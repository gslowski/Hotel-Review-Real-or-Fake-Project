{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Project.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABCcJUJaymgp",
        "outputId": "b7f74047-b126-4c29-f6f7-97b8834d6de3"
      },
      "source": [
        "!which pip\n",
        "!pwd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/global/software/jupyterhub-spark/anaconda3/bin/pip\n",
            "/home/gregory.slowski\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_3GoCVhymgq",
        "outputId": "8d205315-cf8e-4c6e-e0e5-8ef654e03d24"
      },
      "source": [
        "import os\n",
        "import atexit\n",
        "import sys\n",
        "import time\n",
        "import pyspark\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "import findspark\n",
        "from sparkhpc import sparkjob\n",
        "\n",
        "#Exit handler to clean up the Spark cluster if the script exits or crashes\n",
        "def exitHandler(sj,sc):\n",
        "    try:\n",
        "        print('Trapped Exit cleaning up Spark Context')\n",
        "        sc.stop()\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        print('Trapped Exit cleaning up Spark Job')\n",
        "        sj.stop()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "#Parameters for the Spark cluster\n",
        "nodes=1\n",
        "tasks_per_node=4\n",
        "memory_per_task=4096 #4 gig per process, adjust accordingly\n",
        "# Please estimate walltime carefully to keep unused Spark clusters from sitting \n",
        "# idle so that others may use the resources.\n",
        "walltime=\"3:00\" #1 hours\n",
        "#os.environ['SBATCH_PARTITION']='cpu2019' #Set the appropriate ARC partition\n",
        "\n",
        "sj = sparkjob.sparkjob(\n",
        "     ncores=nodes*tasks_per_node,\n",
        "     cores_per_executor=tasks_per_node,\n",
        "     memory_per_core=memory_per_task,\n",
        "     walltime=walltime\n",
        "    )\n",
        "\n",
        "sj.wait_to_start()\n",
        "time.sleep(60)\n",
        "sc = sj.start_spark()\n",
        "\n",
        "#Register the exit handler                                                                                                     \n",
        "atexit.register(exitHandler,sj,sc)\n",
        "\n",
        "#You need this line if you want to use SparkSQL\n",
        "sqlCtx=SQLContext(sc)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:sparkhpc.sparkjob:Submitted batch job 12453\n",
            "\n",
            "INFO:sparkhpc.sparkjob:Submitted cluster 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLsp9nYpymgq"
      },
      "source": [
        "sqlContext = sqlCtx\n",
        "spark = sqlCtx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTe06vP5ymgr"
      },
      "source": [
        "#!pip install --upgrade pip\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import udf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BX17T63eymgr"
      },
      "source": [
        "def tokenize1(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    return words  \n",
        "tokenize_word = udf(lambda x: tokenize1(x)  , ArrayType(StringType()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIlta-GJymgr"
      },
      "source": [
        "def tokenize2(text):\n",
        "    sents = nltk.sent_tokenize(text)\n",
        "    return sents  \n",
        "tokenize_sent = udf(lambda x: tokenize2(x)  , ArrayType(StringType()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ7KXv5Symgr"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_en = stopwords.words('english')\n",
        "def remove_stopwords1(word_list):\n",
        "    filtered_words = [word for word in word_list if word not in stop_en]\n",
        "    return filtered_words\n",
        "remove_stopwords = udf(lambda x: remove_stopwords1(x) , ArrayType(StringType()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RToRXi8fymgr"
      },
      "source": [
        "def remove_noise1(word_list):\n",
        "    filtered_words = [word for word in word_list if word.isalnum() and len(word)>2]\n",
        "    return filtered_words\n",
        "remove_noise = udf(lambda x: remove_noise1(x) , ArrayType(StringType()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91Olkdiwymgs"
      },
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "def stem1(word_list):\n",
        "    snowball = SnowballStemmer(language='english')\n",
        "    stemmed_words = [snowball.stem(word) for word in word_list]\n",
        "    return stemmed_words\n",
        "stem = udf(lambda x: stem1(x) , ArrayType(StringType()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJUre_G9ymgs"
      },
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "def sentiment1(text):\n",
        "  sia = SentimentIntensityAnalyzer()\n",
        "  return sia.polarity_scores(text)['compound']\n",
        "sentiment = udf(lambda x: sentiment1(x) , FloatType())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGFXg2gI5GnA"
      },
      "source": [
        "#area for testing functions:\n",
        "\n",
        "mylist = ['GeeksforGeeks', \",\", 'is', 'a', 'portal', 'for', 'geeks']\n",
        "alphanum_words = [word for word in mylist if word.isalnum()]\n",
        "N = len(alphanum_words)\n",
        "punctuation_words = [word for word in mylist if not(word.isalnum())]\n",
        "num_punctuation_words = len(punctuation_words)\n",
        "ratio_punctuation_to_total = num_punctuation_words / N\n",
        "ratio_punctuation_to_total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2F9Z5-x1MQS"
      },
      "source": [
        "#greg\n",
        "#create udf for adding length of reviews by word count for all alphanumeric \"words\"\n",
        "\n",
        "def get_length1(word_list):\n",
        "  alphanum_words = [word for word in word_list if word.isalnum()]\n",
        "  N = len(alphanum_words)\n",
        "  return N\n",
        "\n",
        "get_length = udf(lambda x: get_length1(x), IntegerType())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0AoKhsg2xGr"
      },
      "source": [
        "#greg\n",
        "#create udf for adding average word length by review\n",
        "\n",
        "def get_average_word_length1(word_list):\n",
        "  alphanum_word_lengths = [len(word) for word in word_list if word.isalnum()]\n",
        "  avg_word_len = sum(alphanum_word_lengths)/len(alphanum_word_lengths)\n",
        "  return avg_word_len\n",
        "\n",
        "get_average_word_length = udf(lambda x: get_average_word_length1(x), FloatType())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKhA9adF4h2f"
      },
      "source": [
        "#greg\n",
        "#create udf for ratio of capitalized characters to total characters (n), with no white spaces\n",
        "\n",
        "def capital_ratio1(word_list):\n",
        "  s = \"\".join(word_list)\n",
        "  n = len(s)\n",
        "  cap_n = len([char for char in s if char.isupper()])\n",
        "  ratio_cap_to_total = cap_n / n\n",
        "  return ratio_cap_to_total\n",
        "\n",
        "capital_ratio = udf(lambda x: capital_ratio1(x), FloatType())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD1h-42o8Tff"
      },
      "source": [
        "#greg\n",
        "#create udf for ratio of long words (>5 letters) to total words (N)\n",
        "\n",
        "def long_word_ratio1(word_list):\n",
        "  alphanum_words = [word for word in word_list if word.isalnum()]\n",
        "  N = len(alphanum_words)\n",
        "  long_words = [word for word in alphanum_words if len(word) > 5]\n",
        "  long_words_count = len(long_words)\n",
        "  ratio_long_to_total = long_words_count / N\n",
        "  return ratio_long_to_total\n",
        "\n",
        "long_word_ratio = udf(lambda x: long_word_ratio1(x), FloatType())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGml4bBK90Fu"
      },
      "source": [
        "#greg\n",
        "#create udf for ratio of words after stop word removal vs total words\n",
        "\n",
        "stop_en = stopwords.words('english')\n",
        "\n",
        "def filtered_word_ratio1(word_list):\n",
        "  alphanum_words = [word for word in word_list if word.isalnum()]\n",
        "  N = len(alphanum_words)\n",
        "  non_filtered_words = [word for word in alphanum_words if word not in stop_en]\n",
        "  num_non_filtered_words = len(non_filtered_words)\n",
        "  ratio_non_filtered_to_total = num_non_filtered_words / N\n",
        "  return ratio_non_filtered_to_total\n",
        "\n",
        "filtered_word_ratio = udf(lambda x: filtered_word_ratio1(x), FloatType())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL1BEle3_XSS"
      },
      "source": [
        "#greg\n",
        "#create udf for ratio of punctuation \"words\" vs total alphanumeric words\n",
        "\n",
        "def punctuation_word_ratio1(word_list):\n",
        "  alphanum_words = [word for word in word_list if word.isalnum()]\n",
        "  N = len(alphanum_words)\n",
        "  punctuation_words = [word for word in word_list if not(word.isalnum())]\n",
        "  num_punctuation_words = len(punctuation_words)\n",
        "  ratio_punctuation_to_total = num_punctuation_words / N\n",
        "  return ratio_punctuation_to_total\n",
        "\n",
        "punctuation_word_ratio = udf(lambda x: punctuation_word_ratio1(x), FloatType())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTMzKHxCymgs"
      },
      "source": [
        "from pyspark.sql.functions import lower\n",
        "def process_data(df):\n",
        "  dfText = df.select(\"Index\",\"Review\" ,\"polarity\",\"real_fake\", tokenize_word(\"Review\").alias(\"tokenized_words\"))\n",
        "  dfText = dfText.withColumn(\"sentiment\" ,sentiment(\"Review\"))\n",
        "  dfText = dfText.withColumn(\"tokenized_sents\" ,tokenize_sent(\"Review\"))\n",
        "  dfText = dfText.withColumn(\"no_stopwords\", remove_stopwords(\"tokenized_words\"))\n",
        "  dfText = dfText.withColumn(\"no_noise\", remove_noise(\"no_stopwords\"))\n",
        "  dfText = dfText.withColumn(\"stemmed\", stem(\"no_noise\"))\n",
        "  #dfText = dfText.select('*', F.concat_ws(\"_\",\"real_fake\",\"polarity\").alias(\"target\"))\n",
        "  dfText = dfText.withColumn(\"length_in_words\", get_length(\"tokenized_words\"))\n",
        "  dfText = dfText.withColumn(\"average_word_length\", get_average_word_length(\"tokenized_words\"))\n",
        "  dfText = dfText.withColumn(\"capital_char_ratio\", capital_ratio(\"tokenized_words\"))\n",
        "  dfText = dfText.withColumn(\"long_word_ratio\", long_word_ratio(\"tokenized_words\"))\n",
        "  dfText = dfText.withColumn(\"non_stop_word_ratio\", filtered_word_ratio(\"tokenized_words\"))\n",
        "  dfText = dfText.withColumn(\"punctuation_ratio\", punctuation_word_ratio(\"tokenized_words\"))\n",
        "\n",
        "  return dfText"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4N3PTiiymgs"
      },
      "source": [
        "df_raw_test = spark.read.option(\"escape\",\"\\\"\").option(\"header\",True).csv(\"Hotel_Reviews_Calgary.csv\")\n",
        "df_test_all = process_data(df_raw_test)\n",
        "#df_test = df_test_all.select(\"Review\",\"stemmed\",\"sentiment\",\"target\",\"real_fake\")\n",
        "df_test = df_test_all.select(\"Review\",\"stemmed\",\"sentiment\",\"polarity\", \"length_in_words\", \"average_word_length\", \\\n",
        "                             \"capital_char_ratio\", \"long_word_ratio\", \"non_stop_word_ratio\", \"punctuation_ratio\", \"real_fake\")\n",
        "\n",
        "df_raw_train = spark.read.option(\"escape\",\"\\\"\").option(\"header\",True).csv(\"Original_data.csv\")\n",
        "df_train = process_data(df_raw_train)\n",
        "#df_train = df_train.select(\"Review\",\"stemmed\",\"sentiment\",\"target\",\"real_fake\")\n",
        "df_train = df_train.select(\"Review\",\"stemmed\",\"sentiment\",\"polarity\", \"length_in_words\", \"average_word_length\", \\\n",
        "                             \"capital_char_ratio\", \"long_word_ratio\", \"non_stop_word_ratio\", \"punctuation_ratio\", \"real_fake\")\n",
        "\n",
        "#df_train.display()\n",
        "combined = df_train.union(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhemXRN6ymgt"
      },
      "source": [
        "print((df_test.count(), len(df_test.columns)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U1IVAYFymgt"
      },
      "source": [
        "df_train.cache()\n",
        "df_test.cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk1XLB55ymgt"
      },
      "source": [
        "df_train.show(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGYen5-symgt"
      },
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, IndexToString, StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "train_split, test_split = df_train.randomSplit(weights = [0.80, 0.20], seed = 1)\n",
        "train_split.cache()\n",
        "\n",
        "columns_to_be_scaled = ['length_in_words', 'average_word_length', 'capital_char_ratio', 'long_word_ratio', 'non_stop_word_ratio', 'punctuation_ratio']\n",
        "assemblers = [VectorAssembler(inputCols=[col], outputCol=col + \"_vec\") for col in columns_to_be_scaled]\n",
        "scalers  = [StandardScaler(inputCol=col + \"_vec\", outputCol=col + \"_scaled\") for col in columns_to_be_scaled]\n",
        "scaling_pipeline = Pipeline(stages=assemblers + scalers)\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"stemmed\", outputCol=\"rawFeatures\", numFeatures=20000)\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"TF_IDF\", minDocFreq=2)\n",
        "label_strIdx1 = StringIndexer(inputCol=\"polarity\", outputCol=\"polarity_idx\")\n",
        "assembles = VectorAssembler(inputCols = ['TF_IDF','sentiment','polarity_idx', 'length_in_words_scaled', 'average_word_length_scaled', 'capital_char_ratio_scaled', 'long_word_ratio_scaled', 'non_stop_word_ratio_scaled', 'punctuation_ratio_scaled'],outputCol=\"features\")\n",
        "label_strIdx2 = StringIndexer(inputCol=\"real_fake\", outputCol=\"label\")\n",
        "lr = LogisticRegression(regParam = 0.3)\n",
        "label_idxStr = IndexToString(inputCol = \"label\", outputCol = \"article_class\")\n",
        "\n",
        "pipeline = Pipeline(stages=[scaling_pipeline, hashingTF, idf,label_strIdx1, assembles, label_strIdx2, lr ,label_idxStr])\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH6rivzgymgu"
      },
      "source": [
        "model = pipeline.fit(train_split)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqeM-MJPymgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6b5aa14-d5d2-423e-d0af-f473294a936f"
      },
      "source": [
        "pred = model.transform(test_split)\n",
        "pred_original = model.transform(df_test)\n",
        "pred.cache()\n",
        "pred_original.cache()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Review: string, stemmed: array<string>, sentiment: float, polarity: string, length_in_words: int, average_word_length: float, capital_char_ratio: float, long_word_ratio: float, non_stop_word_ratio: float, punctuation_ratio: float, real_fake: string, length_in_words_vec: vector, average_word_length_vec: vector, capital_char_ratio_vec: vector, long_word_ratio_vec: vector, non_stop_word_ratio_vec: vector, punctuation_ratio_vec: vector, length_in_words_scaled: vector, average_word_length_scaled: vector, capital_char_ratio_scaled: vector, long_word_ratio_scaled: vector, non_stop_word_ratio_scaled: vector, punctuation_ratio_scaled: vector, rawFeatures: vector, TF_IDF: vector, polarity_idx: double, features: vector, label: double, rawPrediction: vector, probability: vector, prediction: double, article_class: string]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv8sBpuJymgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38169c50-500a-43dd-f8e0-b39d1a77be27"
      },
      "source": [
        "pred.select(\"review\",\"polarity\", \"label\", \"prediction\",\"article_class\").show()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------+-----+----------+-------------+\n",
            "|              review|polarity|label|prediction|article_class|\n",
            "+--------------------+--------+-----+----------+-------------+\n",
            "| My wife and I's ...|positive|  0.0|       0.0|         fake|\n",
            "|A bunch of us got...|positive|  1.0|       0.0|         real|\n",
            "|A lovely hotel in...|negative|  1.0|       1.0|         real|\n",
            "|A recent stay at ...|negative|  0.0|       0.0|         fake|\n",
            "|Affinia hotel in ...|negative|  0.0|       0.0|         fake|\n",
            "|After considering...|negative|  0.0|       0.0|         fake|\n",
            "|After reading so ...|positive|  1.0|       1.0|         real|\n",
            "|After reading the...|negative|  1.0|       1.0|         real|\n",
            "|After reading the...|positive|  1.0|       1.0|         real|\n",
            "|After some delibe...|positive|  1.0|       1.0|         real|\n",
            "|All I can say is ...|negative|  1.0|       0.0|         real|\n",
            "|Amalifa Hotel in ...|positive|  0.0|       0.0|         fake|\n",
            "|Awesome hotel, ou...|positive|  1.0|       1.0|         real|\n",
            "|Booked a room onl...|positive|  1.0|       1.0|         real|\n",
            "|Chicago has many ...|negative|  0.0|       0.0|         fake|\n",
            "|Conrad Chicago is...|positive|  0.0|       0.0|         fake|\n",
            "|Couldn't have ask...|positive|  0.0|       0.0|         fake|\n",
            "|DO NOT STAY HERE!...|negative|  0.0|       0.0|         fake|\n",
            "|Don't ever stay a...|negative|  0.0|       0.0|         fake|\n",
            "|Downtown Chicago ...|positive|  0.0|       0.0|         fake|\n",
            "+--------------------+--------+-----+----------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bFAWwH5ymgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bd3be53-19fe-4809-e6a2-d4dd23368ea6"
      },
      "source": [
        "pred.show()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+---------+--------+---------------+-------------------+------------------+---------------+-------------------+-----------------+---------+-------------------+-----------------------+----------------------+--------------------+-----------------------+---------------------+----------------------+--------------------------+-------------------------+----------------------+--------------------------+------------------------+--------------------+--------------------+------------+--------------------+-----+--------------------+--------------------+----------+-------------+\n",
            "|              Review|             stemmed|sentiment|polarity|length_in_words|average_word_length|capital_char_ratio|long_word_ratio|non_stop_word_ratio|punctuation_ratio|real_fake|length_in_words_vec|average_word_length_vec|capital_char_ratio_vec| long_word_ratio_vec|non_stop_word_ratio_vec|punctuation_ratio_vec|length_in_words_scaled|average_word_length_scaled|capital_char_ratio_scaled|long_word_ratio_scaled|non_stop_word_ratio_scaled|punctuation_ratio_scaled|         rawFeatures|              TF_IDF|polarity_idx|            features|label|       rawPrediction|         probability|prediction|article_class|\n",
            "+--------------------+--------------------+---------+--------+---------------+-------------------+------------------+---------------+-------------------+-----------------+---------+-------------------+-----------------------+----------------------+--------------------+-----------------------+---------------------+----------------------+--------------------------+-------------------------+----------------------+--------------------------+------------------------+--------------------+--------------------+------------+--------------------+-----+--------------------+--------------------+----------+-------------+\n",
            "| My wife and I's ...|[wife, stay, sher...|   0.9744|positive|            248|           4.278226|       0.030541012|     0.23790322|         0.53629035|       0.12903225|     fake|            [248.0]|    [4.278225898742676]|  [0.03054101206362...|[0.23790322244167...|   [0.5362903475761414]| [0.12903225421905...|  [2.8803515145980305]|      [16.049619309523358]|     [1.0284257078117143]|   [4.961675448900411]|      [11.042779376586633]|     [2.676099206262784]|(20000,[1,76,494,...|(20000,[1,76,494,...|         1.0|(20008,[1,76,494,...|  0.0|[2.63992114503762...|[0.93338706173354...|       0.0|         fake|\n",
            "|A bunch of us got...|[bunch, got, toge...|   0.9714|positive|             65|          4.0615387|        0.00754717|     0.13846155|         0.47692308|      0.015384615|     real|             [65.0]|   [4.0615386962890625]|  [0.00754716992378...|[0.13846154510974...|   [0.4769230782985687]| [0.01538461539894...|  [0.7549308405196451]|      [15.236724621178785]|     [0.25414035247660...|  [2.8877341043847764]|       [9.820345186253633]|    [0.3190733767068506]|(20000,[399,494,1...|(20000,[399,494,1...|         1.0|(20008,[399,494,1...|  1.0|[0.39400744724328...|[0.59724703725796...|       0.0|         real|\n",
            "|A lovely hotel in...|[love, hotel, tre...|  -0.2756|negative|            128|          4.6640625|         0.0109375|      0.3046875|           0.578125|        0.1484375|     real|            [128.0]|            [4.6640625]|  [0.01093749981373...|         [0.3046875]|             [0.578125]|          [0.1484375]|  [1.4866330397925318]|      [17.497072228659846]|     [0.3683049521803317]|  [6.3545187527145925]|      [11.904198641544161]|    [3.0785595302028717]|(20000,[93,103,49...|(20000,[93,103,49...|         0.0|(20008,[93,103,49...|  1.0|[-1.4078081075094...|[0.19658000555414...|       1.0|         real|\n",
            "|A recent stay at ...|[recent, stay, ja...|  -0.4512|negative|            266|          4.2593985|       0.020833334|     0.22180451|         0.53759396|       0.13157895|     fake|            [266.0]|    [4.259398460388184]|  [0.02083333395421...|[0.22180451452732...|   [0.5375939607620239]| [0.1315789520740509]|   [3.089409285818855]|      [15.978988813304838]|     [0.7015332751026112]|   [4.625923108104589]|      [11.069622136052981]|    [2.7289171326765564]|(20000,[76,390,49...|(20000,[76,390,49...|         0.0|(20008,[76,390,49...|  0.0|[1.09756190078510...|[0.74980300055859...|       0.0|         fake|\n",
            "|Affinia hotel in ...|[affinia, hotel, ...|  -0.2479|negative|            145|           4.303448|       0.026687598|      0.2137931|         0.56551725|      0.089655176|     fake|            [145.0]|     [4.30344820022583]|  [0.02668759785592...|[0.2137930989265442]|    [0.565517246723175]| [0.08965517580509...|  [1.6840764903899774]|      [16.144239917807184]|      [0.898667393785091]|   [4.458838174620524]|      [11.644591810096111]|    [1.8594276776877774]|(20000,[1,192,494...|(20000,[1,192,494...|         0.0|(20008,[1,192,494...|  0.0|[1.61684946206694...|[0.83436017370022...|       0.0|         fake|\n",
            "|After considering...|[after, consid, s...|  -0.7778|negative|             81|           4.716049|        0.02284264|     0.32098764|          0.6296296|       0.14814815|     fake|             [81.0]|   [4.7160491943359375]|  [0.02284264005720...|[0.32098764181137...|   [0.6296296119689941]| [0.14814814925193...|  [0.9407599704937115]|      [17.692098548681322]|     [0.7691938374596899]|   [6.694472169944585]|      [12.964732491203938]|    [3.0725584623930664]|(20000,[1,512,179...|(20000,[1,512,179...|         0.0|(20008,[1,512,179...|  0.0|[0.50892532912891...|[0.62455451275886...|       0.0|         fake|\n",
            "|After reading so ...|[after, read, man...|   0.9667|positive|            114|           4.359649|       0.036053132|     0.27192983|         0.57894737|        0.1491228|     real|            [114.0]|    [4.359649181365967]|  [0.0360531322658062]|[0.2719298303127289]|   [0.5789473652839661]| [0.14912280440330...|  [1.3240325510652236]|      [16.355075991794298]|     [1.2140386177134848]|   [5.671329497090405]|      [11.921132003180903]|     [3.092772585541885]|(20000,[1,47,158,...|(20000,[1,47,158,...|         1.0|(20008,[1,47,158,...|  1.0|[-2.5088902224539...|[0.07523728768384...|       1.0|         real|\n",
            "|After reading the...|[after, read, tri...|   0.8778|negative|            124|            4.41129|       0.033043478|     0.29032257|          0.5483871|       0.19354838|     real|            [124.0]|    [4.411290168762207]|  [0.03304347768425...|[0.29032257199287...|   [0.5483871102333069]| [0.19354838132858...|  [1.4401757572990153]|       [16.54880540396048]|     [1.1126927246288634]|   [6.054925876726322]|      [11.291864376526979]|     [4.014148809394176]|(20000,[344,504,7...|(20000,[344,504,7...|         0.0|(20008,[344,504,7...|  1.0|[-0.3934362074744...|[0.40289037810747...|       1.0|         real|\n",
            "|After reading the...|[after, read, rev...|   0.9253|positive|             75|               4.04|        0.03215434|     0.18666667|         0.53333336|       0.10666667|     real|             [75.0]|    [4.039999961853027]|  [0.03215434029698...|[0.18666666746139...|   [0.5333333611488342]| [0.1066666692495346]|  [0.8710740467534366]|      [15.155922789697918]|     [1.0827522712821969]|  [3.8930932148190363]|      [10.981891932156772]|    [2.2122421300086055]|(20000,[344,419,1...|(20000,[344,419,1...|         1.0|(20008,[344,419,1...|  1.0|[-0.4602934933825...|[0.38691620138218...|       1.0|         real|\n",
            "|After some delibe...|[after, deliber, ...|    0.996|positive|            178|           4.247191|        0.04336735|     0.20786516|          0.5898876|       0.10674157|     real|            [178.0]|    [4.247190952301025]|  [0.04336734861135...|[0.2078651636838913]|   [0.5898876190185547]| [0.10674156993627...|  [2.0673490709614897]|      [15.933192760886381]|     [1.4603345854629786]|   [4.335206008337643]|       [12.14640327435139]|    [2.2137955529845033]|(20000,[1,76,494,...|(20000,[1,76,494,...|         1.0|(20008,[1,76,494,...|  1.0|[-0.6287273516453...|[0.34779916293102...|       1.0|         real|\n",
            "|All I can say is ...|[all, say, avoid,...|  -0.9098|negative|             80|              4.025|       0.063253015|         0.2125|                0.6|            0.125|     real|             [80.0]|    [4.025000095367432]|  [0.0632530152797699]|[0.21250000596046...|   [0.6000000238418579]|              [0.125]|  [0.9291456498703323]|       [15.09965129948553]|      [2.129956495050309]|   [4.431869613383333]|      [12.354628270261118]|     [2.592471183328734]|(20000,[494,1882,...|(20000,[494,1882,...|         0.0|(20008,[494,1882,...|  1.0|[0.37724328337122...|[0.59320804471983...|       0.0|         real|\n",
            "|Amalifa Hotel in ...|[amalifa, hotel, ...|   0.8764|positive|             37|           5.027027|       0.026455026|      0.3783784|          0.5945946|       0.08108108|     fake|             [37.0]|    [5.027027130126953]|  [0.02645502611994...|[0.37837839126586...|   [0.5945945978164673]| [0.0810810774564743]|  [0.42972986306502...|       [18.85872172409009]|      [0.890835867059911]|  [7.8914054003559215]|       [12.24332489270709]|    [1.6816028545532375]|(20000,[504,2067,...|(20000,[504,2067,...|         1.0|(20008,[504,4418,...|  0.0|[0.68908302432146...|[0.66576290991175...|       0.0|         fake|\n",
            "|Awesome hotel, ou...|[awesom, hotel, r...|   0.9052|positive|             51|           4.352941|       0.030172413|     0.25490198|          0.5686275|       0.19607843|     real|             [51.0]|    [4.352941036224365]|  [0.03017241321504...|[0.2549019753932953]|   [0.5686274766921997]| [0.19607843458652...|  [0.5923303517923368]|      [16.329910612886334]|     [1.0160136590242956]|   [5.316198999764311]|      [11.708634699388485]|     [4.066621530702193]|(20000,[494,525,2...|(20000,[494,525,2...|         1.0|(20008,[494,525,2...|  1.0|[-1.5056089974837...|[0.18159045141442...|       1.0|         real|\n",
            "|Booked a room onl...|[book, room, onli...|   0.9926|positive|            137|           4.072993|       0.013840831|     0.18978103|          0.5182482|       0.13868614|     real|            [137.0]|     [4.07299280166626]|  [0.01384083088487...|[0.18978102505207...|   [0.5182482004165649]| [0.1386861354112625]|   [1.591161925402944]|      [15.279694308891912]|     [0.46607055030874...|  [3.9580458095681004]|       [10.67127268909246]|    [2.8763184766473984]|(20000,[93,494,76...|(20000,[93,494,76...|         1.0|(20008,[93,494,76...|  1.0|[-1.3731768446419...|[0.20210706425817...|       1.0|         real|\n",
            "|Chicago has many ...|[chicago, mani, w...|  -0.4049|negative|            257|           4.365759|       0.030927835|     0.26848248|          0.6108949|       0.15564202|     fake|            [257.0]|    [4.365758895874023]|  [0.03092783503234...|[0.2684824764728546]|   [0.6108949184417725]| [0.15564201772212...|   [2.984880400208443]|       [16.37799637848379]|      [1.041451428261855]|  [5.5994319803799275]|      [12.578965549389455]|    [3.2279795668780937]|(20000,[1,68,93,1...|(20000,[1,68,93,1...|         0.0|(20008,[1,68,93,1...|  0.0|[2.80411312949842...|[0.94289768559056...|       0.0|         fake|\n",
            "|Conrad Chicago is...|[conrad, chicago,...|   0.6808|positive|             38|          4.4210525|       0.029069768|     0.28947368|         0.55263156|       0.10526316|     fake|             [38.0]|      [4.4210524559021]|  [0.02906976826488...|[0.28947368264198...|   [0.5526315569877625]| [0.10526315867900...|  [0.4413441836884079]|       [16.58542829295556]|     [0.9788836382194119]|   [6.037221562308377]|      [11.379261976161136]|    [2.1831336443319156]|(20000,[494,1067,...|(20000,[494,1067,...|         1.0|(20008,[494,1067,...|  0.0|[0.58256383938514...|[0.64165713213205...|       0.0|         fake|\n",
            "|Couldn't have ask...|[could, ask, bett...|   0.9736|positive|            116|          4.5603447|       0.021778584|      0.2672414|          0.5862069|       0.14655173|     fake|            [116.0]|    [4.560344696044922]|  [0.02177858352661...|[0.26724138855934...|   [0.5862069129943848]| [0.1465517282485962]|   [1.347261192311982]|      [17.107978406010606]|     [0.7333632275130049]|   [5.573548029052134]|      [12.070613686195115]|     [3.039449058812074]|(20000,[76,103,49...|(20000,[76,103,49...|         1.0|(20008,[76,103,49...|  0.0|[0.80203854180483...|[0.69041037602605...|       0.0|         fake|\n",
            "|DO NOT STAY HERE!...|[not, stay, here,...|   -0.742|negative|             99|          4.1616163|        0.06398104|     0.22222222|          0.5858586|        0.1010101|     fake|             [99.0]|    [4.161616325378418]|  [0.06398104131221...|[0.2222222238779068]|   [0.5858585834503174]| [0.10101009905338...|  [1.1498177417145363]|      [15.612162451321389]|     [2.1544717496910453]|   [4.634634794345351]|      [12.063441216425954]|    [2.0949261681686093]|(20000,[93,360,49...|(20000,[93,360,49...|         0.0|(20008,[93,360,49...|  0.0|[1.62403920469460...|[0.83535143245233...|       0.0|         fake|\n",
            "|Don't ever stay a...|[ever, stay, hote...|  -0.8614|negative|             99|           4.010101|       0.049411766|     0.18181819|          0.5555556|        0.2020202|     fake|             [99.0]|    [4.010100841522217]|  [0.04941176623106...|[0.1818181872367859]|   [0.5555555820465088]| [0.20202019810676...|  [1.1498177417145363]|       [15.04375725417999]|     [1.6638718636614014]|   [3.791974007403418]|      [11.439470711524887]|     [4.189852336337219]|(20000,[76,494,94...|(20000,[76,494,94...|         0.0|(20008,[76,494,94...|  0.0|[0.65029753630074...|[0.65707750866624...|       0.0|         fake|\n",
            "|Downtown Chicago ...|[downtown, chicag...|   0.9702|positive|            118|          4.8728814|       0.020618556|     0.38135594|         0.60169494|      0.059322033|     fake|            [118.0]|   [4.8728814125061035]|  [0.02061855606734...|[0.3813559412956238]|   [0.6016949415206909]| [0.05932203307747...|  [1.3704898335587403]|       [18.28044929421793]|     [0.6943009312672499]|  [7.9535047562573675]|       [12.38952839865877]|     [1.230325290318694]|(20000,[103,1041,...|(20000,[103,1041,...|         1.0|(20008,[103,1041,...|  0.0|[2.22632826395142...|[0.90258900916717...|       0.0|         fake|\n",
            "+--------------------+--------------------+---------+--------+---------------+-------------------+------------------+---------------+-------------------+-----------------+---------+-------------------+-----------------------+----------------------+--------------------+-----------------------+---------------------+----------------------+--------------------------+-------------------------+----------------------+--------------------------+------------------------+--------------------+--------------------+------------+--------------------+-----+--------------------+--------------------+----------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGux6YdQymgv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8b6f69f-7ab0-46d7-fde0-84b05196758d"
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "eval = MulticlassClassificationEvaluator(labelCol=\"label\",predictionCol=\"prediction\",metricName=\"accuracy\")\n",
        "acc_our_data = eval.evaluate(pred)\n",
        "print(\"our data: \", acc_our_data)\n",
        "acc_original_data = eval.evaluate(pred_original)\n",
        "print(\"original data: \", acc_original_data)\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "our data:  0.8621700879765396\n",
            "original data:  0.609\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-NRE3_Xymgv"
      },
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(hashingTF.numFeatures, [10000,20000,50000]) \\\n",
        "    .addGrid(lr.regParam, [0.1, 0.3 ,0.5]) \\\n",
        "    .build()\n",
        "crossval = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid,evaluator= MulticlassClassificationEvaluator(),numFolds=4,parallelism = 100 )  # use 3+ folds in practice\n",
        "\n",
        "### ON REFINEMENT DO 5 FOLD? MAYBE JUST FOR BEST PRELIMINARY MODEL?###"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwNf3KNLymgv"
      },
      "source": [
        "cvModel = crossval.fit(train_split)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IAzifUZymgv"
      },
      "source": [
        "p = cvModel.transform(test_split)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2csZqvzKymgv"
      },
      "source": [
        "acc = eval.evaluate(p)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMwgXgVmymgv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28135254-9eb3-444c-a5c2-e8113a015d3c"
      },
      "source": [
        "acc"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8709677419354839"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esqtuOF6ymgv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "b4852580-5ca1-4031-ab9c-023af9ca4682"
      },
      "source": [
        "params = [{\n",
        "      p.name: v\n",
        "      for p,\n",
        "      v in m.items()\n",
        "   }\n",
        "   for m in cvModel.getEstimatorParamMaps()\n",
        "]\n",
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame.from_dict([{\n",
        "      cvModel.getEvaluator().getMetricName(): metric,\n",
        "      ** ps\n",
        "   }\n",
        "   for ps, metric in zip(params, cvModel.avgMetrics)\n",
        "])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         f1  numFeatures  regParam\n",
              "0  0.835169        10000       0.1\n",
              "1  0.839130        10000       0.3\n",
              "2  0.841498        10000       0.5\n",
              "3  0.834417        20000       0.1\n",
              "4  0.840727        20000       0.3\n",
              "5  0.847895        20000       0.5\n",
              "6  0.840545        50000       0.1\n",
              "7  0.845282        50000       0.3\n",
              "8  0.846079        50000       0.5"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>numFeatures</th>\n",
              "      <th>regParam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.835169</td>\n",
              "      <td>10000</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.839130</td>\n",
              "      <td>10000</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.841498</td>\n",
              "      <td>10000</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.834417</td>\n",
              "      <td>20000</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.840727</td>\n",
              "      <td>20000</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.847895</td>\n",
              "      <td>20000</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.840545</td>\n",
              "      <td>50000</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.845282</td>\n",
              "      <td>50000</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.846079</td>\n",
              "      <td>50000</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HZrDboiymgw"
      },
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, IndexToString, StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "train_split, test_split = df_train.randomSplit(weights = [0.80, 0.20], seed = 1)\n",
        "train_split.cache()\n",
        "\n",
        "columns_to_be_scaled = ['length_in_words', 'average_word_length', 'capital_char_ratio', 'long_word_ratio', 'non_stop_word_ratio', 'punctuation_ratio']\n",
        "assemblers = [VectorAssembler(inputCols=[col], outputCol=col + \"_vec\") for col in columns_to_be_scaled]\n",
        "scalers  = [StandardScaler(inputCol=col + \"_vec\", outputCol=col + \"_scaled\") for col in columns_to_be_scaled]\n",
        "scaling_pipeline = Pipeline(stages=assemblers + scalers)\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"stemmed\", outputCol=\"rawFeatures\")\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"TF_IDF\", minDocFreq=2)\n",
        "label_strIdx1 = StringIndexer(inputCol=\"polarity\", outputCol=\"polarity_idx\")\n",
        "assembles = VectorAssembler(inputCols = ['TF_IDF','sentiment','polarity_idx'],outputCol=\"features\")\n",
        "label_strIdx2 = StringIndexer(inputCol=\"real_fake\", outputCol=\"label\")\n",
        "svc = LinearSVC(featuresCol=\"features\",labelCol=\"label\")\n",
        "label_idxStr = IndexToString(inputCol = \"label\", outputCol = \"article_class\")\n",
        "\n",
        "pipeline_svc = Pipeline(stages=[scaling_pipeline, hashingTF, idf,label_strIdx1 , assembles, label_strIdx2, svc ,label_idxStr])"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn-VtpOkymgw"
      },
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(hashingTF.numFeatures, [10000,20000,50000]) \\\n",
        "    .addGrid(svc.regParam, [0.0001, 0.001, 0.01, 0.1, 1]) \\\n",
        "    .build()\n",
        "crossval_svc = CrossValidator(estimator=pipeline_svc, estimatorParamMaps=paramGrid,evaluator= MulticlassClassificationEvaluator(),numFolds=2,parallelism = 100 )  # use 3+ folds in practice"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqS5YpWpymgw"
      },
      "source": [
        "cvModel_svc = crossval_svc.fit(train_split)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af7bBGIrymgw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dd84d6b-886e-4906-8711-ce6db1709254"
      },
      "source": [
        "pred_svc = cvModel_svc.transform(test_split)\n",
        "pred_original_svc = cvModel_svc.transform(df_test)\n",
        "pred_svc.cache()\n",
        "pred_original_svc.cache()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Review: string, stemmed: array<string>, sentiment: float, polarity: string, length_in_words: int, average_word_length: float, capital_char_ratio: float, long_word_ratio: float, non_stop_word_ratio: float, punctuation_ratio: float, real_fake: string, length_in_words_vec: vector, average_word_length_vec: vector, capital_char_ratio_vec: vector, long_word_ratio_vec: vector, non_stop_word_ratio_vec: vector, punctuation_ratio_vec: vector, length_in_words_scaled: vector, average_word_length_scaled: vector, capital_char_ratio_scaled: vector, long_word_ratio_scaled: vector, non_stop_word_ratio_scaled: vector, punctuation_ratio_scaled: vector, rawFeatures: vector, TF_IDF: vector, polarity_idx: double, features: vector, label: double, rawPrediction: vector, prediction: double, article_class: string]"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFJbAweqymgw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b9a2d82-0ea4-4b95-b8ec-89030d5995c1"
      },
      "source": [
        "eval = MulticlassClassificationEvaluator(labelCol=\"label\",predictionCol=\"prediction\",metricName=\"accuracy\")\n",
        "acc_our_data_svc = eval.evaluate(pred_svc)\n",
        "print(\"our data: \", acc_our_data_svc)\n",
        "acc_original_data_svc = eval.evaluate(pred_original_svc)\n",
        "print(\"original data: \", acc_original_data_svc)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "our data:  0.8621700879765396\n",
            "original data:  0.625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ-u8rPzymgw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "98e55727-a720-45ce-a543-ce08bd654da2"
      },
      "source": [
        "params = [{\n",
        "      p.name: v\n",
        "      for p,\n",
        "      v in m.items()\n",
        "   }\n",
        "   for m in cvModel_svc.getEstimatorParamMaps()\n",
        "]\n",
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame.from_dict([{\n",
        "      cvModel_svc.getEvaluator().getMetricName(): metric,\n",
        "      ** ps\n",
        "   }\n",
        "   for ps, metric in zip(params, cvModel_svc.avgMetrics)\n",
        "])"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         f1  numFeatures  regParam\n",
              "0  0.835169        10000    0.0001\n",
              "1  0.839130        10000    0.0010\n",
              "2  0.841498        10000    0.0100\n",
              "3  0.834417        10000    0.1000\n",
              "4  0.840727        10000    1.0000\n",
              "5  0.847895        20000    0.0001\n",
              "6  0.840545        20000    0.0010\n",
              "7  0.845282        20000    0.0100\n",
              "8  0.846079        20000    0.1000"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>numFeatures</th>\n",
              "      <th>regParam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.835169</td>\n",
              "      <td>10000</td>\n",
              "      <td>0.0001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.839130</td>\n",
              "      <td>10000</td>\n",
              "      <td>0.0010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.841498</td>\n",
              "      <td>10000</td>\n",
              "      <td>0.0100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.834417</td>\n",
              "      <td>10000</td>\n",
              "      <td>0.1000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.840727</td>\n",
              "      <td>10000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.847895</td>\n",
              "      <td>20000</td>\n",
              "      <td>0.0001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.840545</td>\n",
              "      <td>20000</td>\n",
              "      <td>0.0010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.845282</td>\n",
              "      <td>20000</td>\n",
              "      <td>0.0100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.846079</td>\n",
              "      <td>20000</td>\n",
              "      <td>0.1000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AziTYK3ymgx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ff039ec-6d88-4512-f98a-5c943de5844a"
      },
      "source": [
        "svc.params"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Param(parent='LinearSVC_19c554e505fc', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'),\n",
              " Param(parent='LinearSVC_19c554e505fc', name='featuresCol', doc='features column name.'),\n",
              " Param(parent='LinearSVC_19c554e505fc', name='fitIntercept', doc='whether to fit an intercept term.'),\n",
              " Param(parent='LinearSVC_19c554e505fc', name='labelCol', doc='label column name.'),\n",
              " Param(parent='LinearSVC_19c554e505fc', name='maxIter', doc='max number of iterations (>= 0).'),\n",
              " Param(parent='LinearSVC_19c554e505fc', name='predictionCol', doc='prediction column name.'),\n",
              " Param(parent='LinearSVC_19c554e505fc', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'),\n",
              " Param(parent='LinearSVC_19c554e505fc', name='regParam', doc='regularization parameter (>= 0).'),\n",
              " Param(parent='LinearSVC_19c554e505fc', name='standardization', doc='whether to standardize the training features before fitting the model.'),\n",
              " Param(parent='LinearSVC_19c554e505fc', name='threshold', doc='The threshold in binary classification applied to the linear model prediction.  This threshold can be any real number, where Inf will make all predictions 0.0 and -Inf will make all predictions 1.0.'),\n",
              " Param(parent='LinearSVC_19c554e505fc', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'),\n",
              " Param(parent='LinearSVC_19c554e505fc', name='weightCol', doc='weight column name. If this is not set or empty, we treat all instance weights as 1.0.')]"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTYWEIrvymgx"
      },
      "source": [
        "#greg\n",
        "\n",
        "from pyspark.ml.feature import HashingTF, IDF, IndexToString, StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "train_split, test_split = df_train.randomSplit(weights = [0.80, 0.20], seed = 1)\n",
        "train_split.cache()\n",
        "\n",
        "columns_to_be_scaled = ['length_in_words', 'average_word_length', 'capital_char_ratio', 'long_word_ratio', 'non_stop_word_ratio', 'punctuation_ratio']\n",
        "assemblers = [VectorAssembler(inputCols=[col], outputCol=col + \"_vec\") for col in columns_to_be_scaled]\n",
        "scalers  = [StandardScaler(inputCol=col + \"_vec\", outputCol=col + \"_scaled\") for col in columns_to_be_scaled]\n",
        "scaling_pipeline = Pipeline(stages=assemblers + scalers)\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"stemmed\", outputCol=\"rawFeatures\")\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"TF_IDF\", minDocFreq=2)\n",
        "label_strIdx1 = StringIndexer(inputCol=\"polarity\", outputCol=\"polarity_idx\")\n",
        "assembles = VectorAssembler(inputCols = ['TF_IDF','sentiment','polarity_idx'],outputCol=\"features\")\n",
        "label_strIdx2 = StringIndexer(inputCol=\"real_fake\", outputCol=\"label\")\n",
        "rfc = RandomForestClassifier(featuresCol=\"features\",labelCol=\"label\")\n",
        "label_idxStr = IndexToString(inputCol = \"label\", outputCol = \"article_class\")\n",
        "\n",
        "pipeline_rfc = Pipeline(stages=[scaling_pipeline, hashingTF, idf,label_strIdx1 , assembles, label_strIdx2, rfc ,label_idxStr])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYVbeEqdymgx"
      },
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(hashingTF.numFeatures, [10000,20000,50000]) \\\n",
        "    .addGrid(rfc.maxDepth, [3, 5, 9, 15]) \\\n",
        "    .addGrid(rfc.numTrees, [20, 50, 100, 200]) \\\n",
        "    .build()\n",
        "crossval_rfc = CrossValidator(estimator=pipeline_rfc, estimatorParamMaps=paramGrid,evaluator= MulticlassClassificationEvaluator(),numFolds=2,parallelism = 100 )  # use 3+ folds in practice"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OckAS0jMymgx"
      },
      "source": [
        "cvModel_rfc = crossval_rfc.fit(train_split)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sI-0Qbn3_Jnh",
        "outputId": "bb99425f-bdae-4030-f0b9-627b33e799e4"
      },
      "source": [
        "pred_rfc = cvModel_rfc.transform(test_split)\n",
        "pred_original_rfc = cvModel_rfc.transform(df_test)\n",
        "pred_rfc.cache()\n",
        "pred_original_rfc.cache()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Review: string, stemmed: array<string>, sentiment: float, polarity: string, length_in_words: int, average_word_length: float, capital_char_ratio: float, long_word_ratio: float, non_stop_word_ratio: float, punctuation_ratio: float, real_fake: string, length_in_words_vec: vector, average_word_length_vec: vector, capital_char_ratio_vec: vector, long_word_ratio_vec: vector, non_stop_word_ratio_vec: vector, punctuation_ratio_vec: vector, length_in_words_scaled: vector, average_word_length_scaled: vector, capital_char_ratio_scaled: vector, long_word_ratio_scaled: vector, non_stop_word_ratio_scaled: vector, punctuation_ratio_scaled: vector, rawFeatures: vector, TF_IDF: vector, polarity_idx: double, features: vector, label: double, rawPrediction: vector, probability: vector, prediction: double, article_class: string]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0TjecAq_m5R",
        "outputId": "dad11281-0e42-4de9-84f4-a1ff4911ee75"
      },
      "source": [
        "eval = MulticlassClassificationEvaluator(labelCol=\"label\",predictionCol=\"prediction\",metricName=\"accuracy\")\n",
        "acc_our_data_rfc = eval.evaluate(pred_rfc)\n",
        "print(\"our data: \", acc_our_data_rfc)\n",
        "acc_original_data_rfc = eval.evaluate(pred_original_rfc)\n",
        "print(\"original data: \", acc_original_data_rfc)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "our data:  0.8123167155425219\n",
            "original data:  0.588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F2NXdB_0_w_R",
        "outputId": "567ee6b7-acd3-455e-a202-6e35bb1412f3"
      },
      "source": [
        "params = [{\n",
        "      p.name: v\n",
        "      for p,\n",
        "      v in m.items()\n",
        "   }\n",
        "   for m in cvModel_rfc.getEstimatorParamMaps()\n",
        "]\n",
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame.from_dict([{\n",
        "      cvModel_rfc.getEvaluator().getMetricName(): metric,\n",
        "      ** ps\n",
        "   } for ps, metric in zip(params, cvModel_rfc.avgMetrics)\n",
        "])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          f1  maxDepth  numFeatures  numTrees\n",
              "0   0.579049         3        10000         5\n",
              "1   0.670132         3        10000        10\n",
              "2   0.702024         3        10000        20\n",
              "3   0.769181         3        10000        50\n",
              "4   0.610245         5        10000         5\n",
              "5   0.682026         5        10000        10\n",
              "6   0.734050         5        10000        20\n",
              "7   0.793750         5        10000        50\n",
              "8   0.648336         9        10000         5\n",
              "9   0.678103         9        10000        10\n",
              "10  0.736973         9        10000        20\n",
              "11  0.807326         9        10000        50\n",
              "12  0.643232        15        10000         5\n",
              "13  0.713673        15        10000        10\n",
              "14  0.750437        15        10000        20\n",
              "15  0.804373        15        10000        50\n",
              "16  0.563693         3        20000         5\n",
              "17  0.623311         3        20000        10\n",
              "18  0.687898         3        20000        20\n",
              "19  0.739054         3        20000        50\n",
              "20  0.627805         5        20000         5\n",
              "21  0.667810         5        20000        10\n",
              "22  0.721819         5        20000        20\n",
              "23  0.766078         5        20000        50\n",
              "24  0.632750         9        20000         5\n",
              "25  0.705736         9        20000        10\n",
              "26  0.734949         9        20000        20\n",
              "27  0.792228         9        20000        50\n",
              "28  0.658623        15        20000         5\n",
              "29  0.683070        15        20000        10\n",
              "30  0.768767        15        20000        20\n",
              "31  0.793072        15        20000        50\n",
              "32  0.577047         3        50000         5\n",
              "33  0.602210         3        50000        10\n",
              "34  0.626514         3        50000        20\n",
              "35  0.670835         3        50000        50\n",
              "36  0.597269         5        50000         5\n",
              "37  0.635518         5        50000        10\n",
              "38  0.638836         5        50000        20\n",
              "39  0.724345         5        50000        50\n",
              "40  0.626343         9        50000         5\n",
              "41  0.665506         9        50000        10\n",
              "42  0.696204         9        50000        20\n",
              "43  0.747701         9        50000        50\n",
              "44  0.639495        15        50000         5\n",
              "45  0.693061        15        50000        10\n",
              "46  0.706492        15        50000        20\n",
              "47  0.762709        15        50000        50"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>maxDepth</th>\n",
              "      <th>numFeatures</th>\n",
              "      <th>numTrees</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.579049</td>\n",
              "      <td>3</td>\n",
              "      <td>10000</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.670132</td>\n",
              "      <td>3</td>\n",
              "      <td>10000</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.702024</td>\n",
              "      <td>3</td>\n",
              "      <td>10000</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.769181</td>\n",
              "      <td>3</td>\n",
              "      <td>10000</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.610245</td>\n",
              "      <td>5</td>\n",
              "      <td>10000</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.682026</td>\n",
              "      <td>5</td>\n",
              "      <td>10000</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.734050</td>\n",
              "      <td>5</td>\n",
              "      <td>10000</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.793750</td>\n",
              "      <td>5</td>\n",
              "      <td>10000</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.648336</td>\n",
              "      <td>9</td>\n",
              "      <td>10000</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.678103</td>\n",
              "      <td>9</td>\n",
              "      <td>10000</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.736973</td>\n",
              "      <td>9</td>\n",
              "      <td>10000</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.807326</td>\n",
              "      <td>9</td>\n",
              "      <td>10000</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.643232</td>\n",
              "      <td>15</td>\n",
              "      <td>10000</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.713673</td>\n",
              "      <td>15</td>\n",
              "      <td>10000</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.750437</td>\n",
              "      <td>15</td>\n",
              "      <td>10000</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.804373</td>\n",
              "      <td>15</td>\n",
              "      <td>10000</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.563693</td>\n",
              "      <td>3</td>\n",
              "      <td>20000</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.623311</td>\n",
              "      <td>3</td>\n",
              "      <td>20000</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.687898</td>\n",
              "      <td>3</td>\n",
              "      <td>20000</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.739054</td>\n",
              "      <td>3</td>\n",
              "      <td>20000</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.627805</td>\n",
              "      <td>5</td>\n",
              "      <td>20000</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.667810</td>\n",
              "      <td>5</td>\n",
              "      <td>20000</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.721819</td>\n",
              "      <td>5</td>\n",
              "      <td>20000</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.766078</td>\n",
              "      <td>5</td>\n",
              "      <td>20000</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.632750</td>\n",
              "      <td>9</td>\n",
              "      <td>20000</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.705736</td>\n",
              "      <td>9</td>\n",
              "      <td>20000</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.734949</td>\n",
              "      <td>9</td>\n",
              "      <td>20000</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.792228</td>\n",
              "      <td>9</td>\n",
              "      <td>20000</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.658623</td>\n",
              "      <td>15</td>\n",
              "      <td>20000</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.683070</td>\n",
              "      <td>15</td>\n",
              "      <td>20000</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.768767</td>\n",
              "      <td>15</td>\n",
              "      <td>20000</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.793072</td>\n",
              "      <td>15</td>\n",
              "      <td>20000</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.577047</td>\n",
              "      <td>3</td>\n",
              "      <td>50000</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.602210</td>\n",
              "      <td>3</td>\n",
              "      <td>50000</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.626514</td>\n",
              "      <td>3</td>\n",
              "      <td>50000</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.670835</td>\n",
              "      <td>3</td>\n",
              "      <td>50000</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.597269</td>\n",
              "      <td>5</td>\n",
              "      <td>50000</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.635518</td>\n",
              "      <td>5</td>\n",
              "      <td>50000</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.638836</td>\n",
              "      <td>5</td>\n",
              "      <td>50000</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.724345</td>\n",
              "      <td>5</td>\n",
              "      <td>50000</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.626343</td>\n",
              "      <td>9</td>\n",
              "      <td>50000</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.665506</td>\n",
              "      <td>9</td>\n",
              "      <td>50000</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.696204</td>\n",
              "      <td>9</td>\n",
              "      <td>50000</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.747701</td>\n",
              "      <td>9</td>\n",
              "      <td>50000</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.639495</td>\n",
              "      <td>15</td>\n",
              "      <td>50000</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.693061</td>\n",
              "      <td>15</td>\n",
              "      <td>50000</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.706492</td>\n",
              "      <td>15</td>\n",
              "      <td>50000</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.762709</td>\n",
              "      <td>15</td>\n",
              "      <td>50000</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0NiNweqAN8W"
      },
      "source": [
        "#greg\n",
        "\n",
        "from pyspark.ml.feature import HashingTF, IDF, IndexToString, StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "train_split, test_split = df_train.randomSplit(weights = [0.80, 0.20], seed = 1)\n",
        "train_split.cache()\n",
        "\n",
        "columns_to_be_scaled = ['length_in_words', 'average_word_length', 'capital_char_ratio', 'long_word_ratio', 'non_stop_word_ratio', 'punctuation_ratio']\n",
        "assemblers = [VectorAssembler(inputCols=[col], outputCol=col + \"_vec\") for col in columns_to_be_scaled]\n",
        "scalers  = [StandardScaler(inputCol=col + \"_vec\", outputCol=col + \"_scaled\") for col in columns_to_be_scaled]\n",
        "scaling_pipeline = Pipeline(stages=assemblers + scalers)\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"stemmed\", outputCol=\"rawFeatures\")\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"TF_IDF\", minDocFreq=2)\n",
        "label_strIdx1 = StringIndexer(inputCol=\"polarity\", outputCol=\"polarity_idx\")\n",
        "assembles = VectorAssembler(inputCols = ['TF_IDF','sentiment','polarity_idx'],outputCol=\"features\")\n",
        "label_strIdx2 = StringIndexer(inputCol=\"real_fake\", outputCol=\"label\")\n",
        "gbtc = GBTClassifier(featuresCol=\"features\",labelCol=\"label\")\n",
        "label_idxStr = IndexToString(inputCol = \"label\", outputCol = \"article_class\")\n",
        "\n",
        "pipeline_gbtc = Pipeline(stages=[scaling_pipeline, hashingTF, idf,label_strIdx1 , assembles, label_strIdx2, gbtc ,label_idxStr])"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aEGVcyLBy51"
      },
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(hashingTF.numFeatures, [10000,20000,50000]) \\\n",
        "    .addGrid(gbtc.maxDepth, [3, 5, 9, 15]) \\\n",
        "    .addGrid(gbtc.stepSize, [0.01, 0.1, 0.5, 1]) \\\n",
        "    .build()\n",
        "crossval_gbtc = CrossValidator(estimator=pipeline_gbtc, estimatorParamMaps=paramGrid,evaluator= MulticlassClassificationEvaluator(),numFolds=2,parallelism = 100 )  # use 3+ folds in practice"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eb-J_DdCq1I"
      },
      "source": [
        "cvModel_gbtc = crossval_gbtc.fit(train_split)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdH_7sRnC6Dg"
      },
      "source": [
        "pred_gbtc = cvModel_gbtc.transform(test_split)\n",
        "pred_original_gbtc = cvModel_gbtc.transform(df_test)\n",
        "pred_gbtc.cache()\n",
        "pred_original_gbtc.cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_91JR4vDC-xA"
      },
      "source": [
        "eval = MulticlassClassificationEvaluator(labelCol=\"label\",predictionCol=\"prediction\",metricName=\"accuracy\")\n",
        "acc_our_data_gbtc = eval.evaluate(pred_gbtc)\n",
        "print(\"our data: \", acc_our_data_gbtc)\n",
        "acc_original_data_gbtc = eval.evaluate(pred_original_gbtc)\n",
        "print(\"original data: \", acc_original_data_gbtc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsxqI7FYDKtv"
      },
      "source": [
        "params = [{\n",
        "      p.name: v\n",
        "      for p,\n",
        "      v in m.items()\n",
        "   }\n",
        "   for m in cvModel_gbtc.getEstimatorParamMaps()\n",
        "]\n",
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame.from_dict([{\n",
        "      cvModel_gbtc.getEvaluator().getMetricName(): metric,\n",
        "      ** ps\n",
        "   }\n",
        "   for ps, metric in zip(params, cvModel_gbtc.avgMetrics)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DaBrnnNDeJe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}