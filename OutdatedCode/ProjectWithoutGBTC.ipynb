{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Project.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABCcJUJaymgp",
        "outputId": "8092ba61-1b4e-44af-df4f-028d088a9d8e"
      },
      "source": [
        "!which pip\n",
        "!pwd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/global/software/jupyterhub-spark/anaconda3/bin/pip\n",
            "/home/gregory.slowski\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_3GoCVhymgq",
        "outputId": "3747a678-b889-4414-e069-fa6bc303d8a0"
      },
      "source": [
        "import os\n",
        "import atexit\n",
        "import sys\n",
        "import time\n",
        "import pyspark\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "import findspark\n",
        "from sparkhpc import sparkjob\n",
        "\n",
        "#Exit handler to clean up the Spark cluster if the script exits or crashes\n",
        "def exitHandler(sj,sc):\n",
        "    try:\n",
        "        print('Trapped Exit cleaning up Spark Context')\n",
        "        sc.stop()\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        print('Trapped Exit cleaning up Spark Job')\n",
        "        sj.stop()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "#Parameters for the Spark cluster\n",
        "nodes=1\n",
        "tasks_per_node=4\n",
        "memory_per_task=4096 #4 gig per process, adjust accordingly\n",
        "# Please estimate walltime carefully to keep unused Spark clusters from sitting \n",
        "# idle so that others may use the resources.\n",
        "walltime=\"15:00\" #12 hours\n",
        "#os.environ['SBATCH_PARTITION']='cpu2019' #Set the appropriate ARC partition\n",
        "\n",
        "sj = sparkjob.sparkjob(\n",
        "     ncores=nodes*tasks_per_node,\n",
        "     cores_per_executor=tasks_per_node,\n",
        "     memory_per_core=memory_per_task,\n",
        "     walltime=walltime\n",
        "    )\n",
        "\n",
        "sj.wait_to_start()\n",
        "time.sleep(60)\n",
        "sc = sj.start_spark()\n",
        "\n",
        "#Register the exit handler                                                                                                     \n",
        "atexit.register(exitHandler,sj,sc)\n",
        "\n",
        "#You need this line if you want to use SparkSQL\n",
        "sqlCtx=SQLContext(sc)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:sparkhpc.sparkjob:Submitted batch job 12474\n",
            "\n",
            "INFO:sparkhpc.sparkjob:Submitted cluster 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLsp9nYpymgq"
      },
      "source": [
        "sqlContext = sqlCtx\n",
        "spark = sqlCtx"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTe06vP5ymgr",
        "outputId": "637542d4-56ef-4232-d9fd-76a6cef0d95a"
      },
      "source": [
        "#!pip install --upgrade pip\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import udf"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /global/software/jupyterhub-spark/anaconda3/lib/python3.7/site-packages (3.3)\r\n",
            "Requirement already satisfied: six in /global/software/jupyterhub-spark/anaconda3/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
            "\u001b[33mYou are using pip version 10.0.1, however version 21.3.1 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /home/gregory.slowski/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BX17T63eymgr"
      },
      "source": [
        "def tokenize1(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    return words  \n",
        "tokenize_word = udf(lambda x: tokenize1(x)  , ArrayType(StringType()))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIlta-GJymgr"
      },
      "source": [
        "def tokenize2(text):\n",
        "    sents = nltk.sent_tokenize(text)\n",
        "    return sents  \n",
        "tokenize_sent = udf(lambda x: tokenize2(x)  , ArrayType(StringType()))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQ7KXv5Symgr",
        "outputId": "8326c5f7-771e-4297-aa6f-51f926475e63"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_en = stopwords.words('english')\n",
        "def remove_stopwords1(word_list):\n",
        "    filtered_words = [word for word in word_list if word not in stop_en]\n",
        "    return filtered_words\n",
        "remove_stopwords = udf(lambda x: remove_stopwords1(x) , ArrayType(StringType()))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/gregory.slowski/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /home/gregory.slowski/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RToRXi8fymgr"
      },
      "source": [
        "def remove_noise1(word_list):\n",
        "    filtered_words = [word for word in word_list if word.isalnum() and len(word)>2]\n",
        "    return filtered_words\n",
        "remove_noise = udf(lambda x: remove_noise1(x) , ArrayType(StringType()))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91Olkdiwymgs"
      },
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "def stem1(word_list):\n",
        "    snowball = SnowballStemmer(language='english')\n",
        "    stemmed_words = [snowball.stem(word) for word in word_list]\n",
        "    return stemmed_words\n",
        "stem = udf(lambda x: stem1(x) , ArrayType(StringType()))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJUre_G9ymgs",
        "outputId": "d7f6df02-fe07-481b-9733-c0c4cbbc257a"
      },
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "def sentiment1(text):\n",
        "  sia = SentimentIntensityAnalyzer()\n",
        "  return sia.polarity_scores(text)['compound']\n",
        "sentiment = udf(lambda x: sentiment1(x) , FloatType())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     /home/gregory.slowski/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/global/software/jupyterhub-spark/anaconda3/lib/python3.7/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGFXg2gI5GnA",
        "outputId": "04599f37-e17d-4240-83fa-3fca1f2f2c58"
      },
      "source": [
        "#area for testing functions:\n",
        "\n",
        "mylist = ['GeeksforGeeks', \",\", 'is', 'a', 'portal', 'for', 'geeks']\n",
        "alphanum_words = [word for word in mylist if word.isalnum()]\n",
        "N = len(alphanum_words)\n",
        "punctuation_words = [word for word in mylist if not(word.isalnum())]\n",
        "num_punctuation_words = len(punctuation_words)\n",
        "ratio_punctuation_to_total = num_punctuation_words / N\n",
        "ratio_punctuation_to_total"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.16666666666666666"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2F9Z5-x1MQS"
      },
      "source": [
        "#greg\n",
        "#create udf for adding length of reviews by word count for all alphanumeric \"words\"\n",
        "\n",
        "def get_length1(word_list):\n",
        "  alphanum_words = [word for word in word_list if word.isalnum()]\n",
        "  N = len(alphanum_words)\n",
        "  return N\n",
        "\n",
        "get_length = udf(lambda x: get_length1(x), IntegerType())"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0AoKhsg2xGr"
      },
      "source": [
        "#greg\n",
        "#create udf for adding average word length by review\n",
        "\n",
        "def get_average_word_length1(word_list):\n",
        "  alphanum_word_lengths = [len(word) for word in word_list if word.isalnum()]\n",
        "  avg_word_len = sum(alphanum_word_lengths)/len(alphanum_word_lengths)\n",
        "  return avg_word_len\n",
        "\n",
        "get_average_word_length = udf(lambda x: get_average_word_length1(x), FloatType())"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKhA9adF4h2f"
      },
      "source": [
        "#greg\n",
        "#create udf for ratio of capitalized characters to total characters (n), with no white spaces\n",
        "\n",
        "def capital_ratio1(word_list):\n",
        "  s = \"\".join(word_list)\n",
        "  n = len(s)\n",
        "  cap_n = len([char for char in s if char.isupper()])\n",
        "  ratio_cap_to_total = cap_n / n\n",
        "  return ratio_cap_to_total\n",
        "\n",
        "capital_ratio = udf(lambda x: capital_ratio1(x), FloatType())\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD1h-42o8Tff"
      },
      "source": [
        "#greg\n",
        "#create udf for ratio of long words (>5 letters) to total words (N)\n",
        "\n",
        "def long_word_ratio1(word_list):\n",
        "  alphanum_words = [word for word in word_list if word.isalnum()]\n",
        "  N = len(alphanum_words)\n",
        "  long_words = [word for word in alphanum_words if len(word) > 5]\n",
        "  long_words_count = len(long_words)\n",
        "  ratio_long_to_total = long_words_count / N\n",
        "  return ratio_long_to_total\n",
        "\n",
        "long_word_ratio = udf(lambda x: long_word_ratio1(x), FloatType())"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGml4bBK90Fu"
      },
      "source": [
        "#greg\n",
        "#create udf for ratio of words after stop word removal vs total words\n",
        "\n",
        "stop_en = stopwords.words('english')\n",
        "\n",
        "def filtered_word_ratio1(word_list):\n",
        "  alphanum_words = [word for word in word_list if word.isalnum()]\n",
        "  N = len(alphanum_words)\n",
        "  non_filtered_words = [word for word in alphanum_words if word not in stop_en]\n",
        "  num_non_filtered_words = len(non_filtered_words)\n",
        "  ratio_non_filtered_to_total = num_non_filtered_words / N\n",
        "  return ratio_non_filtered_to_total\n",
        "\n",
        "filtered_word_ratio = udf(lambda x: filtered_word_ratio1(x), FloatType())\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL1BEle3_XSS"
      },
      "source": [
        "#greg\n",
        "#create udf for ratio of punctuation \"words\" vs total alphanumeric words\n",
        "\n",
        "def punctuation_word_ratio1(word_list):\n",
        "  alphanum_words = [word for word in word_list if word.isalnum()]\n",
        "  N = len(alphanum_words)\n",
        "  punctuation_words = [word for word in word_list if not(word.isalnum())]\n",
        "  num_punctuation_words = len(punctuation_words)\n",
        "  ratio_punctuation_to_total = num_punctuation_words / N\n",
        "  return ratio_punctuation_to_total\n",
        "\n",
        "punctuation_word_ratio = udf(lambda x: punctuation_word_ratio1(x), FloatType())"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTMzKHxCymgs"
      },
      "source": [
        "from pyspark.sql.functions import lower\n",
        "def process_data(df):\n",
        "  dfText = df.select(\"Index\",\"Review\" ,\"polarity\",\"real_fake\", tokenize_word(\"Review\").alias(\"tokenized_words\"))\n",
        "  dfText = dfText.withColumn(\"sentiment\" ,sentiment(\"Review\"))\n",
        "  dfText = dfText.withColumn(\"tokenized_sents\" ,tokenize_sent(\"Review\"))\n",
        "  dfText = dfText.withColumn(\"no_stopwords\", remove_stopwords(\"tokenized_words\"))\n",
        "  dfText = dfText.withColumn(\"no_noise\", remove_noise(\"no_stopwords\"))\n",
        "  dfText = dfText.withColumn(\"stemmed\", stem(\"no_noise\"))\n",
        "  #dfText = dfText.select('*', F.concat_ws(\"_\",\"real_fake\",\"polarity\").alias(\"target\"))\n",
        "  dfText = dfText.withColumn(\"length_in_words\", get_length(\"tokenized_words\"))\n",
        "  dfText = dfText.withColumn(\"average_word_length\", get_average_word_length(\"tokenized_words\"))\n",
        "  dfText = dfText.withColumn(\"capital_char_ratio\", capital_ratio(\"tokenized_words\"))\n",
        "  dfText = dfText.withColumn(\"long_word_ratio\", long_word_ratio(\"tokenized_words\"))\n",
        "  dfText = dfText.withColumn(\"non_stop_word_ratio\", filtered_word_ratio(\"tokenized_words\"))\n",
        "  dfText = dfText.withColumn(\"punctuation_ratio\", punctuation_word_ratio(\"tokenized_words\"))\n",
        "\n",
        "  return dfText"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4N3PTiiymgs"
      },
      "source": [
        "df_raw_test = spark.read.option(\"escape\",\"\\\"\").option(\"header\",True).csv(\"Hotel_Reviews_Calgary.csv\")\n",
        "df_test_all = process_data(df_raw_test)\n",
        "#df_test = df_test_all.select(\"Review\",\"stemmed\",\"sentiment\",\"target\",\"real_fake\")\n",
        "df_test = df_test_all.select(\"Review\",\"stemmed\",\"sentiment\",\"polarity\", \"length_in_words\", \"average_word_length\", \\\n",
        "                             \"capital_char_ratio\", \"long_word_ratio\", \"non_stop_word_ratio\", \"punctuation_ratio\", \"real_fake\")\n",
        "\n",
        "df_raw_train = spark.read.option(\"escape\",\"\\\"\").option(\"header\",True).csv(\"Original_data.csv\")\n",
        "df_train = process_data(df_raw_train)\n",
        "#df_train = df_train.select(\"Review\",\"stemmed\",\"sentiment\",\"target\",\"real_fake\")\n",
        "df_train = df_train.select(\"Review\",\"stemmed\",\"sentiment\",\"polarity\", \"length_in_words\", \"average_word_length\", \\\n",
        "                             \"capital_char_ratio\", \"long_word_ratio\", \"non_stop_word_ratio\", \"punctuation_ratio\", \"real_fake\")\n",
        "\n",
        "#df_train.display()\n",
        "combined = df_train.union(df_test)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhemXRN6ymgt",
        "outputId": "4726ded0-a71c-457c-e28a-285a9effb610"
      },
      "source": [
        "print((df_test.count(), len(df_test.columns)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8U1IVAYFymgt",
        "outputId": "588ae773-7449-4e19-8f94-94a808775f8f"
      },
      "source": [
        "df_train.cache()\n",
        "df_test.cache()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Review: string, stemmed: array<string>, sentiment: float, polarity: string, length_in_words: int, average_word_length: float, capital_char_ratio: float, long_word_ratio: float, non_stop_word_ratio: float, punctuation_ratio: float, real_fake: string]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vk1XLB55ymgt",
        "outputId": "0ad6a61e-dac7-4364-fac0-b210c87e53c0"
      },
      "source": [
        "df_train.show(1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+---------+--------+---------------+-------------------+------------------+---------------+-------------------+-----------------+---------+\n",
            "|              Review|             stemmed|sentiment|polarity|length_in_words|average_word_length|capital_char_ratio|long_word_ratio|non_stop_word_ratio|punctuation_ratio|real_fake|\n",
            "+--------------------+--------------------+---------+--------+---------------+-------------------+------------------+---------------+-------------------+-----------------+---------+\n",
            "|We stayed 2 night...|[stay, night, spr...|  -0.9593|negative|             66|          4.1060605|        0.04693141|     0.25757575|         0.59090906|       0.09090909|     real|\n",
            "+--------------------+--------------------+---------+--------+---------------+-------------------+------------------+---------------+-------------------+-----------------+---------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGYen5-symgt"
      },
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, IndexToString, StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "train_split, test_split = df_train.randomSplit(weights = [0.80, 0.20], seed = 1)\n",
        "train_split.cache()\n",
        "\n",
        "columns_to_be_scaled = ['length_in_words', 'average_word_length', 'capital_char_ratio', 'long_word_ratio', 'non_stop_word_ratio', 'punctuation_ratio']\n",
        "assemblers = [VectorAssembler(inputCols=[col], outputCol=col + \"_vec\") for col in columns_to_be_scaled]\n",
        "scalers  = [StandardScaler(inputCol=col + \"_vec\", outputCol=col + \"_scaled\") for col in columns_to_be_scaled]\n",
        "scaling_pipeline = Pipeline(stages=assemblers + scalers)\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"stemmed\", outputCol=\"rawFeatures\", numFeatures=20000)\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"TF_IDF\", minDocFreq=2)\n",
        "label_strIdx1 = StringIndexer(inputCol=\"polarity\", outputCol=\"polarity_idx\")\n",
        "assembles = VectorAssembler(inputCols = ['TF_IDF','sentiment','polarity_idx', 'length_in_words_scaled', 'average_word_length_scaled', 'capital_char_ratio_scaled', 'long_word_ratio_scaled', 'non_stop_word_ratio_scaled', 'punctuation_ratio_scaled'],outputCol=\"features\")\n",
        "label_strIdx2 = StringIndexer(inputCol=\"real_fake\", outputCol=\"label\")\n",
        "lr = LogisticRegression(regParam = 0.3)\n",
        "label_idxStr = IndexToString(inputCol = \"label\", outputCol = \"article_class\")\n",
        "\n",
        "pipeline = Pipeline(stages=[scaling_pipeline, hashingTF, idf,label_strIdx1, assembles, label_strIdx2, lr ,label_idxStr])\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH6rivzgymgu"
      },
      "source": [
        "model = pipeline.fit(train_split)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqeM-MJPymgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84799f49-9fd5-49fd-9559-a423a5fa389f"
      },
      "source": [
        "pred = model.transform(test_split)\n",
        "pred_original = model.transform(df_test)\n",
        "pred.cache()\n",
        "pred_original.cache()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Review: string, stemmed: array<string>, sentiment: float, polarity: string, length_in_words: int, average_word_length: float, capital_char_ratio: float, long_word_ratio: float, non_stop_word_ratio: float, punctuation_ratio: float, real_fake: string, length_in_words_vec: vector, average_word_length_vec: vector, capital_char_ratio_vec: vector, long_word_ratio_vec: vector, non_stop_word_ratio_vec: vector, punctuation_ratio_vec: vector, length_in_words_scaled: vector, average_word_length_scaled: vector, capital_char_ratio_scaled: vector, long_word_ratio_scaled: vector, non_stop_word_ratio_scaled: vector, punctuation_ratio_scaled: vector, rawFeatures: vector, TF_IDF: vector, polarity_idx: double, features: vector, label: double, rawPrediction: vector, probability: vector, prediction: double, article_class: string]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv8sBpuJymgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86c5287e-2352-4926-91df-e9bd6fac4ae6"
      },
      "source": [
        "pred.select(\"review\",\"polarity\", \"label\", \"prediction\",\"article_class\").show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------+-----+----------+-------------+\n",
            "|              review|polarity|label|prediction|article_class|\n",
            "+--------------------+--------+-----+----------+-------------+\n",
            "| My wife and I's ...|positive|  0.0|       0.0|         fake|\n",
            "|A bunch of us got...|positive|  1.0|       0.0|         real|\n",
            "|A lovely hotel in...|negative|  1.0|       1.0|         real|\n",
            "|A recent stay at ...|negative|  0.0|       0.0|         fake|\n",
            "|Affinia hotel in ...|negative|  0.0|       0.0|         fake|\n",
            "|After considering...|negative|  0.0|       0.0|         fake|\n",
            "|After reading so ...|positive|  1.0|       1.0|         real|\n",
            "|After reading the...|negative|  1.0|       1.0|         real|\n",
            "|After reading the...|positive|  1.0|       1.0|         real|\n",
            "|After some delibe...|positive|  1.0|       1.0|         real|\n",
            "|All I can say is ...|negative|  1.0|       0.0|         real|\n",
            "|Amalifa Hotel in ...|positive|  0.0|       0.0|         fake|\n",
            "|Awesome hotel, ou...|positive|  1.0|       1.0|         real|\n",
            "|Booked a room onl...|positive|  1.0|       1.0|         real|\n",
            "|Chicago has many ...|negative|  0.0|       0.0|         fake|\n",
            "|Conrad Chicago is...|positive|  0.0|       0.0|         fake|\n",
            "|Couldn't have ask...|positive|  0.0|       0.0|         fake|\n",
            "|DO NOT STAY HERE!...|negative|  0.0|       0.0|         fake|\n",
            "|Don't ever stay a...|negative|  0.0|       0.0|         fake|\n",
            "|Downtown Chicago ...|positive|  0.0|       0.0|         fake|\n",
            "+--------------------+--------+-----+----------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bFAWwH5ymgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe04edc6-bb4a-49bb-efb0-fdff8f5dec40"
      },
      "source": [
        "pred.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+---------+--------+---------------+-------------------+------------------+---------------+-------------------+-----------------+---------+-------------------+-----------------------+----------------------+--------------------+-----------------------+---------------------+----------------------+--------------------------+-------------------------+----------------------+--------------------------+------------------------+--------------------+--------------------+------------+--------------------+-----+--------------------+--------------------+----------+-------------+\n",
            "|              Review|             stemmed|sentiment|polarity|length_in_words|average_word_length|capital_char_ratio|long_word_ratio|non_stop_word_ratio|punctuation_ratio|real_fake|length_in_words_vec|average_word_length_vec|capital_char_ratio_vec| long_word_ratio_vec|non_stop_word_ratio_vec|punctuation_ratio_vec|length_in_words_scaled|average_word_length_scaled|capital_char_ratio_scaled|long_word_ratio_scaled|non_stop_word_ratio_scaled|punctuation_ratio_scaled|         rawFeatures|              TF_IDF|polarity_idx|            features|label|       rawPrediction|         probability|prediction|article_class|\n",
            "+--------------------+--------------------+---------+--------+---------------+-------------------+------------------+---------------+-------------------+-----------------+---------+-------------------+-----------------------+----------------------+--------------------+-----------------------+---------------------+----------------------+--------------------------+-------------------------+----------------------+--------------------------+------------------------+--------------------+--------------------+------------+--------------------+-----+--------------------+--------------------+----------+-------------+\n",
            "| My wife and I's ...|[wife, stay, sher...|   0.9744|positive|            248|           4.278226|       0.030541012|     0.23790322|         0.53629035|       0.12903225|     fake|            [248.0]|    [4.278225898742676]|  [0.03054101206362...|[0.23790322244167...|   [0.5362903475761414]| [0.12903225421905...|  [2.8803515145980305]|      [16.049619309523358]|     [1.0284257078117143]|   [4.961675448900411]|      [11.042779376586633]|     [2.676099206262784]|(20000,[1,76,494,...|(20000,[1,76,494,...|         1.0|(20008,[1,76,494,...|  0.0|[2.63992114503762...|[0.93338706173354...|       0.0|         fake|\n",
            "|A bunch of us got...|[bunch, got, toge...|   0.9714|positive|             65|          4.0615387|        0.00754717|     0.13846155|         0.47692308|      0.015384615|     real|             [65.0]|   [4.0615386962890625]|  [0.00754716992378...|[0.13846154510974...|   [0.4769230782985687]| [0.01538461539894...|  [0.7549308405196451]|      [15.236724621178785]|     [0.25414035247660...|  [2.8877341043847764]|       [9.820345186253633]|    [0.3190733767068506]|(20000,[399,494,1...|(20000,[399,494,1...|         1.0|(20008,[399,494,1...|  1.0|[0.39400744724328...|[0.59724703725796...|       0.0|         real|\n",
            "|A lovely hotel in...|[love, hotel, tre...|  -0.2756|negative|            128|          4.6640625|         0.0109375|      0.3046875|           0.578125|        0.1484375|     real|            [128.0]|            [4.6640625]|  [0.01093749981373...|         [0.3046875]|             [0.578125]|          [0.1484375]|  [1.4866330397925318]|      [17.497072228659846]|     [0.3683049521803317]|  [6.3545187527145925]|      [11.904198641544161]|    [3.0785595302028717]|(20000,[93,103,49...|(20000,[93,103,49...|         0.0|(20008,[93,103,49...|  1.0|[-1.4078081075094...|[0.19658000555414...|       1.0|         real|\n",
            "|A recent stay at ...|[recent, stay, ja...|  -0.4512|negative|            266|          4.2593985|       0.020833334|     0.22180451|         0.53759396|       0.13157895|     fake|            [266.0]|    [4.259398460388184]|  [0.02083333395421...|[0.22180451452732...|   [0.5375939607620239]| [0.1315789520740509]|   [3.089409285818855]|      [15.978988813304838]|     [0.7015332751026112]|   [4.625923108104589]|      [11.069622136052981]|    [2.7289171326765564]|(20000,[76,390,49...|(20000,[76,390,49...|         0.0|(20008,[76,390,49...|  0.0|[1.09756190078510...|[0.74980300055859...|       0.0|         fake|\n",
            "|Affinia hotel in ...|[affinia, hotel, ...|  -0.2479|negative|            145|           4.303448|       0.026687598|      0.2137931|         0.56551725|      0.089655176|     fake|            [145.0]|     [4.30344820022583]|  [0.02668759785592...|[0.2137930989265442]|    [0.565517246723175]| [0.08965517580509...|  [1.6840764903899774]|      [16.144239917807184]|      [0.898667393785091]|   [4.458838174620524]|      [11.644591810096111]|    [1.8594276776877774]|(20000,[1,192,494...|(20000,[1,192,494...|         0.0|(20008,[1,192,494...|  0.0|[1.61684946206694...|[0.83436017370022...|       0.0|         fake|\n",
            "|After considering...|[after, consid, s...|  -0.7778|negative|             81|           4.716049|        0.02284264|     0.32098764|          0.6296296|       0.14814815|     fake|             [81.0]|   [4.7160491943359375]|  [0.02284264005720...|[0.32098764181137...|   [0.6296296119689941]| [0.14814814925193...|  [0.9407599704937115]|      [17.692098548681322]|     [0.7691938374596899]|   [6.694472169944585]|      [12.964732491203938]|    [3.0725584623930664]|(20000,[1,512,179...|(20000,[1,512,179...|         0.0|(20008,[1,512,179...|  0.0|[0.50892532912891...|[0.62455451275886...|       0.0|         fake|\n",
            "|After reading so ...|[after, read, man...|   0.9667|positive|            114|           4.359649|       0.036053132|     0.27192983|         0.57894737|        0.1491228|     real|            [114.0]|    [4.359649181365967]|  [0.0360531322658062]|[0.2719298303127289]|   [0.5789473652839661]| [0.14912280440330...|  [1.3240325510652236]|      [16.355075991794298]|     [1.2140386177134848]|   [5.671329497090405]|      [11.921132003180903]|     [3.092772585541885]|(20000,[1,47,158,...|(20000,[1,47,158,...|         1.0|(20008,[1,47,158,...|  1.0|[-2.5088902224539...|[0.07523728768384...|       1.0|         real|\n",
            "|After reading the...|[after, read, tri...|   0.8778|negative|            124|            4.41129|       0.033043478|     0.29032257|          0.5483871|       0.19354838|     real|            [124.0]|    [4.411290168762207]|  [0.03304347768425...|[0.29032257199287...|   [0.5483871102333069]| [0.19354838132858...|  [1.4401757572990153]|       [16.54880540396048]|     [1.1126927246288634]|   [6.054925876726322]|      [11.291864376526979]|     [4.014148809394176]|(20000,[344,504,7...|(20000,[344,504,7...|         0.0|(20008,[344,504,7...|  1.0|[-0.3934362074744...|[0.40289037810747...|       1.0|         real|\n",
            "|After reading the...|[after, read, rev...|   0.9253|positive|             75|               4.04|        0.03215434|     0.18666667|         0.53333336|       0.10666667|     real|             [75.0]|    [4.039999961853027]|  [0.03215434029698...|[0.18666666746139...|   [0.5333333611488342]| [0.1066666692495346]|  [0.8710740467534366]|      [15.155922789697918]|     [1.0827522712821969]|  [3.8930932148190363]|      [10.981891932156772]|    [2.2122421300086055]|(20000,[344,419,1...|(20000,[344,419,1...|         1.0|(20008,[344,419,1...|  1.0|[-0.4602934933825...|[0.38691620138218...|       1.0|         real|\n",
            "|After some delibe...|[after, deliber, ...|    0.996|positive|            178|           4.247191|        0.04336735|     0.20786516|          0.5898876|       0.10674157|     real|            [178.0]|    [4.247190952301025]|  [0.04336734861135...|[0.2078651636838913]|   [0.5898876190185547]| [0.10674156993627...|  [2.0673490709614897]|      [15.933192760886381]|     [1.4603345854629786]|   [4.335206008337643]|       [12.14640327435139]|    [2.2137955529845033]|(20000,[1,76,494,...|(20000,[1,76,494,...|         1.0|(20008,[1,76,494,...|  1.0|[-0.6287273516453...|[0.34779916293102...|       1.0|         real|\n",
            "|All I can say is ...|[all, say, avoid,...|  -0.9098|negative|             80|              4.025|       0.063253015|         0.2125|                0.6|            0.125|     real|             [80.0]|    [4.025000095367432]|  [0.0632530152797699]|[0.21250000596046...|   [0.6000000238418579]|              [0.125]|  [0.9291456498703323]|       [15.09965129948553]|      [2.129956495050309]|   [4.431869613383333]|      [12.354628270261118]|     [2.592471183328734]|(20000,[494,1882,...|(20000,[494,1882,...|         0.0|(20008,[494,1882,...|  1.0|[0.37724328337122...|[0.59320804471983...|       0.0|         real|\n",
            "|Amalifa Hotel in ...|[amalifa, hotel, ...|   0.8764|positive|             37|           5.027027|       0.026455026|      0.3783784|          0.5945946|       0.08108108|     fake|             [37.0]|    [5.027027130126953]|  [0.02645502611994...|[0.37837839126586...|   [0.5945945978164673]| [0.0810810774564743]|  [0.42972986306502...|       [18.85872172409009]|      [0.890835867059911]|  [7.8914054003559215]|       [12.24332489270709]|    [1.6816028545532375]|(20000,[504,2067,...|(20000,[504,2067,...|         1.0|(20008,[504,4418,...|  0.0|[0.68908302432146...|[0.66576290991175...|       0.0|         fake|\n",
            "|Awesome hotel, ou...|[awesom, hotel, r...|   0.9052|positive|             51|           4.352941|       0.030172413|     0.25490198|          0.5686275|       0.19607843|     real|             [51.0]|    [4.352941036224365]|  [0.03017241321504...|[0.2549019753932953]|   [0.5686274766921997]| [0.19607843458652...|  [0.5923303517923368]|      [16.329910612886334]|     [1.0160136590242956]|   [5.316198999764311]|      [11.708634699388485]|     [4.066621530702193]|(20000,[494,525,2...|(20000,[494,525,2...|         1.0|(20008,[494,525,2...|  1.0|[-1.5056089974837...|[0.18159045141442...|       1.0|         real|\n",
            "|Booked a room onl...|[book, room, onli...|   0.9926|positive|            137|           4.072993|       0.013840831|     0.18978103|          0.5182482|       0.13868614|     real|            [137.0]|     [4.07299280166626]|  [0.01384083088487...|[0.18978102505207...|   [0.5182482004165649]| [0.1386861354112625]|   [1.591161925402944]|      [15.279694308891912]|     [0.46607055030874...|  [3.9580458095681004]|       [10.67127268909246]|    [2.8763184766473984]|(20000,[93,494,76...|(20000,[93,494,76...|         1.0|(20008,[93,494,76...|  1.0|[-1.3731768446419...|[0.20210706425817...|       1.0|         real|\n",
            "|Chicago has many ...|[chicago, mani, w...|  -0.4049|negative|            257|           4.365759|       0.030927835|     0.26848248|          0.6108949|       0.15564202|     fake|            [257.0]|    [4.365758895874023]|  [0.03092783503234...|[0.2684824764728546]|   [0.6108949184417725]| [0.15564201772212...|   [2.984880400208443]|       [16.37799637848379]|      [1.041451428261855]|  [5.5994319803799275]|      [12.578965549389455]|    [3.2279795668780937]|(20000,[1,68,93,1...|(20000,[1,68,93,1...|         0.0|(20008,[1,68,93,1...|  0.0|[2.80411312949842...|[0.94289768559056...|       0.0|         fake|\n",
            "|Conrad Chicago is...|[conrad, chicago,...|   0.6808|positive|             38|          4.4210525|       0.029069768|     0.28947368|         0.55263156|       0.10526316|     fake|             [38.0]|      [4.4210524559021]|  [0.02906976826488...|[0.28947368264198...|   [0.5526315569877625]| [0.10526315867900...|  [0.4413441836884079]|       [16.58542829295556]|     [0.9788836382194119]|   [6.037221562308377]|      [11.379261976161136]|    [2.1831336443319156]|(20000,[494,1067,...|(20000,[494,1067,...|         1.0|(20008,[494,1067,...|  0.0|[0.58256383938514...|[0.64165713213205...|       0.0|         fake|\n",
            "|Couldn't have ask...|[could, ask, bett...|   0.9736|positive|            116|          4.5603447|       0.021778584|      0.2672414|          0.5862069|       0.14655173|     fake|            [116.0]|    [4.560344696044922]|  [0.02177858352661...|[0.26724138855934...|   [0.5862069129943848]| [0.1465517282485962]|   [1.347261192311982]|      [17.107978406010606]|     [0.7333632275130049]|   [5.573548029052134]|      [12.070613686195115]|     [3.039449058812074]|(20000,[76,103,49...|(20000,[76,103,49...|         1.0|(20008,[76,103,49...|  0.0|[0.80203854180483...|[0.69041037602605...|       0.0|         fake|\n",
            "|DO NOT STAY HERE!...|[not, stay, here,...|   -0.742|negative|             99|          4.1616163|        0.06398104|     0.22222222|          0.5858586|        0.1010101|     fake|             [99.0]|    [4.161616325378418]|  [0.06398104131221...|[0.2222222238779068]|   [0.5858585834503174]| [0.10101009905338...|  [1.1498177417145363]|      [15.612162451321389]|     [2.1544717496910453]|   [4.634634794345351]|      [12.063441216425954]|    [2.0949261681686093]|(20000,[93,360,49...|(20000,[93,360,49...|         0.0|(20008,[93,360,49...|  0.0|[1.62403920469460...|[0.83535143245233...|       0.0|         fake|\n",
            "|Don't ever stay a...|[ever, stay, hote...|  -0.8614|negative|             99|           4.010101|       0.049411766|     0.18181819|          0.5555556|        0.2020202|     fake|             [99.0]|    [4.010100841522217]|  [0.04941176623106...|[0.1818181872367859]|   [0.5555555820465088]| [0.20202019810676...|  [1.1498177417145363]|       [15.04375725417999]|     [1.6638718636614014]|   [3.791974007403418]|      [11.439470711524887]|     [4.189852336337219]|(20000,[76,494,94...|(20000,[76,494,94...|         0.0|(20008,[76,494,94...|  0.0|[0.65029753630074...|[0.65707750866624...|       0.0|         fake|\n",
            "|Downtown Chicago ...|[downtown, chicag...|   0.9702|positive|            118|          4.8728814|       0.020618556|     0.38135594|         0.60169494|      0.059322033|     fake|            [118.0]|   [4.8728814125061035]|  [0.02061855606734...|[0.3813559412956238]|   [0.6016949415206909]| [0.05932203307747...|  [1.3704898335587403]|       [18.28044929421793]|     [0.6943009312672499]|  [7.9535047562573675]|       [12.38952839865877]|     [1.230325290318694]|(20000,[103,1041,...|(20000,[103,1041,...|         1.0|(20008,[103,1041,...|  0.0|[2.22632826395142...|[0.90258900916717...|       0.0|         fake|\n",
            "+--------------------+--------------------+---------+--------+---------------+-------------------+------------------+---------------+-------------------+-----------------+---------+-------------------+-----------------------+----------------------+--------------------+-----------------------+---------------------+----------------------+--------------------------+-------------------------+----------------------+--------------------------+------------------------+--------------------+--------------------+------------+--------------------+-----+--------------------+--------------------+----------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGux6YdQymgv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "464c3023-e0c9-48dc-a460-1dd29829f2b0"
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "eval = MulticlassClassificationEvaluator(labelCol=\"label\",predictionCol=\"prediction\",metricName=\"accuracy\")\n",
        "acc_our_data = eval.evaluate(pred)\n",
        "print(\"our data: \", acc_our_data)\n",
        "acc_original_data = eval.evaluate(pred_original)\n",
        "print(\"original data: \", acc_original_data)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "our data:  0.8621700879765396\n",
            "original data:  0.609\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-NRE3_Xymgv"
      },
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(hashingTF.numFeatures, [10000,20000,50000]) \\\n",
        "    .addGrid(lr.regParam, [0.1, 0.3 ,0.5]) \\\n",
        "    .build()\n",
        "crossval = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid,evaluator= MulticlassClassificationEvaluator(),numFolds=4,parallelism = 100 )  # use 3+ folds in practice\n",
        "\n",
        "### ON REFINEMENT DO 5 FOLD? MAYBE JUST FOR BEST PRELIMINARY MODEL?###"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwNf3KNLymgv"
      },
      "source": [
        "cvModel = crossval.fit(train_split)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IAzifUZymgv"
      },
      "source": [
        "p = cvModel.transform(test_split)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2csZqvzKymgv"
      },
      "source": [
        "acc = eval.evaluate(p)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMwgXgVmymgv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "201c0ec7-fcf9-44d7-fe62-b86e833d8a5b"
      },
      "source": [
        "acc"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8709677419354839"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esqtuOF6ymgv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "cab45acd-5220-43cb-b542-7cd8d0222510"
      },
      "source": [
        "params = [{\n",
        "      p.name: v\n",
        "      for p,\n",
        "      v in m.items()\n",
        "   }\n",
        "   for m in cvModel.getEstimatorParamMaps()\n",
        "]\n",
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame.from_dict([{\n",
        "      cvModel.getEvaluator().getMetricName(): metric,\n",
        "      ** ps\n",
        "   }\n",
        "   for ps, metric in zip(params, cvModel.avgMetrics)\n",
        "])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         f1  numFeatures  regParam\n",
              "0  0.844959        10000       0.1\n",
              "1  0.853243        10000       0.3\n",
              "2  0.853308        10000       0.5\n",
              "3  0.853482        20000       0.1\n",
              "4  0.850541        20000       0.3\n",
              "5  0.854689        20000       0.5\n",
              "6  0.846758        50000       0.1\n",
              "7  0.854762        50000       0.3\n",
              "8  0.856414        50000       0.5"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>numFeatures</th>\n",
              "      <th>regParam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.844959</td>\n",
              "      <td>10000</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.853243</td>\n",
              "      <td>10000</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.853308</td>\n",
              "      <td>10000</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.853482</td>\n",
              "      <td>20000</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.850541</td>\n",
              "      <td>20000</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.854689</td>\n",
              "      <td>20000</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.846758</td>\n",
              "      <td>50000</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.854762</td>\n",
              "      <td>50000</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.856414</td>\n",
              "      <td>50000</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HZrDboiymgw"
      },
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, IndexToString, StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "train_split, test_split = df_train.randomSplit(weights = [0.80, 0.20], seed = 1)\n",
        "train_split.cache()\n",
        "\n",
        "columns_to_be_scaled = ['length_in_words', 'average_word_length', 'capital_char_ratio', 'long_word_ratio', 'non_stop_word_ratio', 'punctuation_ratio']\n",
        "assemblers = [VectorAssembler(inputCols=[col], outputCol=col + \"_vec\") for col in columns_to_be_scaled]\n",
        "scalers  = [StandardScaler(inputCol=col + \"_vec\", outputCol=col + \"_scaled\") for col in columns_to_be_scaled]\n",
        "scaling_pipeline = Pipeline(stages=assemblers + scalers)\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"stemmed\", outputCol=\"rawFeatures\")\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"TF_IDF\", minDocFreq=2)\n",
        "label_strIdx1 = StringIndexer(inputCol=\"polarity\", outputCol=\"polarity_idx\")\n",
        "assembles = VectorAssembler(inputCols = ['TF_IDF','sentiment','polarity_idx'],outputCol=\"features\")\n",
        "label_strIdx2 = StringIndexer(inputCol=\"real_fake\", outputCol=\"label\")\n",
        "svc = LinearSVC(featuresCol=\"features\",labelCol=\"label\")\n",
        "label_idxStr = IndexToString(inputCol = \"label\", outputCol = \"article_class\")\n",
        "\n",
        "pipeline_svc = Pipeline(stages=[scaling_pipeline, hashingTF, idf,label_strIdx1 , assembles, label_strIdx2, svc ,label_idxStr])"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn-VtpOkymgw"
      },
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(hashingTF.numFeatures, [10000,20000,50000]) \\\n",
        "    .addGrid(svc.regParam, [0.0001, 0.001, 0.01, 0.1, 1]) \\\n",
        "    .build()\n",
        "crossval_svc = CrossValidator(estimator=pipeline_svc, estimatorParamMaps=paramGrid,evaluator= MulticlassClassificationEvaluator(),numFolds=2,parallelism = 100 )  # use 3+ folds in practice"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqS5YpWpymgw"
      },
      "source": [
        "cvModel_svc = crossval_svc.fit(train_split)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af7bBGIrymgw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1124cbc2-c9a4-4280-ccdc-a836b841c5e3"
      },
      "source": [
        "pred_svc = cvModel_svc.transform(test_split)\n",
        "pred_original_svc = cvModel_svc.transform(df_test)\n",
        "pred_svc.cache()\n",
        "pred_original_svc.cache()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Review: string, stemmed: array<string>, sentiment: float, polarity: string, length_in_words: int, average_word_length: float, capital_char_ratio: float, long_word_ratio: float, non_stop_word_ratio: float, punctuation_ratio: float, real_fake: string, length_in_words_vec: vector, average_word_length_vec: vector, capital_char_ratio_vec: vector, long_word_ratio_vec: vector, non_stop_word_ratio_vec: vector, punctuation_ratio_vec: vector, length_in_words_scaled: vector, average_word_length_scaled: vector, capital_char_ratio_scaled: vector, long_word_ratio_scaled: vector, non_stop_word_ratio_scaled: vector, punctuation_ratio_scaled: vector, rawFeatures: vector, TF_IDF: vector, polarity_idx: double, features: vector, label: double, rawPrediction: vector, prediction: double, article_class: string]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFJbAweqymgw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d7f884c-09f0-45f2-dac7-1f2e954b631d"
      },
      "source": [
        "eval = MulticlassClassificationEvaluator(labelCol=\"label\",predictionCol=\"prediction\",metricName=\"accuracy\")\n",
        "acc_our_data_svc = eval.evaluate(pred_svc)\n",
        "print(\"our data: \", acc_our_data_svc)\n",
        "acc_original_data_svc = eval.evaluate(pred_original_svc)\n",
        "print(\"original data: \", acc_original_data_svc)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "our data:  0.844574780058651\n",
            "original data:  0.635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ-u8rPzymgw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "5ee283b3-34cd-4b95-81b9-aaf2a271a74c"
      },
      "source": [
        "params = [{\n",
        "      p.name: v\n",
        "      for p,\n",
        "      v in m.items()\n",
        "   }\n",
        "   for m in cvModel_svc.getEstimatorParamMaps()\n",
        "]\n",
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame.from_dict([{\n",
        "      cvModel_svc.getEvaluator().getMetricName(): metric,\n",
        "      ** ps\n",
        "   }\n",
        "   for ps, metric in zip(params, cvModel_svc.avgMetrics)\n",
        "])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          f1  numFeatures  regParam\n",
              "0   0.827839        10000    0.0001\n",
              "1   0.831910        10000    0.0010\n",
              "2   0.827073        10000    0.0100\n",
              "3   0.821376        10000    0.1000\n",
              "4   0.816114        10000    1.0000\n",
              "5   0.823903        20000    0.0001\n",
              "6   0.826379        20000    0.0010\n",
              "7   0.821700        20000    0.0100\n",
              "8   0.822117        20000    0.1000\n",
              "9   0.816687        20000    1.0000\n",
              "10  0.830294        50000    0.0001\n",
              "11  0.827079        50000    0.0010\n",
              "12  0.831082        50000    0.0100\n",
              "13  0.817141        50000    0.1000\n",
              "14  0.808811        50000    1.0000"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>numFeatures</th>\n",
              "      <th>regParam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.827839</td>\n",
              "      <td>10000</td>\n",
              "      <td>0.0001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.831910</td>\n",
              "      <td>10000</td>\n",
              "      <td>0.0010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.827073</td>\n",
              "      <td>10000</td>\n",
              "      <td>0.0100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.821376</td>\n",
              "      <td>10000</td>\n",
              "      <td>0.1000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.816114</td>\n",
              "      <td>10000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.823903</td>\n",
              "      <td>20000</td>\n",
              "      <td>0.0001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.826379</td>\n",
              "      <td>20000</td>\n",
              "      <td>0.0010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.821700</td>\n",
              "      <td>20000</td>\n",
              "      <td>0.0100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.822117</td>\n",
              "      <td>20000</td>\n",
              "      <td>0.1000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.816687</td>\n",
              "      <td>20000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.830294</td>\n",
              "      <td>50000</td>\n",
              "      <td>0.0001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.827079</td>\n",
              "      <td>50000</td>\n",
              "      <td>0.0010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.831082</td>\n",
              "      <td>50000</td>\n",
              "      <td>0.0100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.817141</td>\n",
              "      <td>50000</td>\n",
              "      <td>0.1000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.808811</td>\n",
              "      <td>50000</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AziTYK3ymgx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37bbc2bb-8844-4487-a806-061661ab731f"
      },
      "source": [
        "svc.params"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Param(parent='LinearSVC_97789cd409da', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'),\n",
              " Param(parent='LinearSVC_97789cd409da', name='featuresCol', doc='features column name.'),\n",
              " Param(parent='LinearSVC_97789cd409da', name='fitIntercept', doc='whether to fit an intercept term.'),\n",
              " Param(parent='LinearSVC_97789cd409da', name='labelCol', doc='label column name.'),\n",
              " Param(parent='LinearSVC_97789cd409da', name='maxIter', doc='max number of iterations (>= 0).'),\n",
              " Param(parent='LinearSVC_97789cd409da', name='predictionCol', doc='prediction column name.'),\n",
              " Param(parent='LinearSVC_97789cd409da', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'),\n",
              " Param(parent='LinearSVC_97789cd409da', name='regParam', doc='regularization parameter (>= 0).'),\n",
              " Param(parent='LinearSVC_97789cd409da', name='standardization', doc='whether to standardize the training features before fitting the model.'),\n",
              " Param(parent='LinearSVC_97789cd409da', name='threshold', doc='The threshold in binary classification applied to the linear model prediction.  This threshold can be any real number, where Inf will make all predictions 0.0 and -Inf will make all predictions 1.0.'),\n",
              " Param(parent='LinearSVC_97789cd409da', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'),\n",
              " Param(parent='LinearSVC_97789cd409da', name='weightCol', doc='weight column name. If this is not set or empty, we treat all instance weights as 1.0.')]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTYWEIrvymgx"
      },
      "source": [
        "#greg\n",
        "\n",
        "from pyspark.ml.feature import HashingTF, IDF, IndexToString, StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "train_split, test_split = df_train.randomSplit(weights = [0.80, 0.20], seed = 1)\n",
        "train_split.cache()\n",
        "\n",
        "columns_to_be_scaled = ['length_in_words', 'average_word_length', 'capital_char_ratio', 'long_word_ratio', 'non_stop_word_ratio', 'punctuation_ratio']\n",
        "assemblers = [VectorAssembler(inputCols=[col], outputCol=col + \"_vec\") for col in columns_to_be_scaled]\n",
        "scalers  = [StandardScaler(inputCol=col + \"_vec\", outputCol=col + \"_scaled\") for col in columns_to_be_scaled]\n",
        "scaling_pipeline = Pipeline(stages=assemblers + scalers)\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"stemmed\", outputCol=\"rawFeatures\")\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"TF_IDF\", minDocFreq=2)\n",
        "label_strIdx1 = StringIndexer(inputCol=\"polarity\", outputCol=\"polarity_idx\")\n",
        "assembles = VectorAssembler(inputCols = ['TF_IDF','sentiment','polarity_idx'],outputCol=\"features\")\n",
        "label_strIdx2 = StringIndexer(inputCol=\"real_fake\", outputCol=\"label\")\n",
        "rfc = RandomForestClassifier(featuresCol=\"features\",labelCol=\"label\")\n",
        "label_idxStr = IndexToString(inputCol = \"label\", outputCol = \"article_class\")\n",
        "\n",
        "pipeline_rfc = Pipeline(stages=[scaling_pipeline, hashingTF, idf,label_strIdx1 , assembles, label_strIdx2, rfc ,label_idxStr])"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYVbeEqdymgx"
      },
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(hashingTF.numFeatures, [10000,20000,50000]) \\\n",
        "    .addGrid(rfc.maxDepth, [3, 5, 9, 15]) \\\n",
        "    .addGrid(rfc.numTrees, [10, 20, 50, 100]) \\\n",
        "    .build()\n",
        "crossval_rfc = CrossValidator(estimator=pipeline_rfc, estimatorParamMaps=paramGrid,evaluator= MulticlassClassificationEvaluator(),numFolds=2,parallelism = 100 )  # use 3+ folds in practice"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OckAS0jMymgx"
      },
      "source": [
        "cvModel_rfc = crossval_rfc.fit(train_split)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sI-0Qbn3_Jnh",
        "outputId": "df98fbd4-9ba8-4e5b-b5a3-1dd4897b4a58"
      },
      "source": [
        "pred_rfc = cvModel_rfc.transform(test_split)\n",
        "pred_original_rfc = cvModel_rfc.transform(df_test)\n",
        "pred_rfc.cache()\n",
        "pred_original_rfc.cache()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Review: string, stemmed: array<string>, sentiment: float, polarity: string, length_in_words: int, average_word_length: float, capital_char_ratio: float, long_word_ratio: float, non_stop_word_ratio: float, punctuation_ratio: float, real_fake: string, length_in_words_vec: vector, average_word_length_vec: vector, capital_char_ratio_vec: vector, long_word_ratio_vec: vector, non_stop_word_ratio_vec: vector, punctuation_ratio_vec: vector, length_in_words_scaled: vector, average_word_length_scaled: vector, capital_char_ratio_scaled: vector, long_word_ratio_scaled: vector, non_stop_word_ratio_scaled: vector, punctuation_ratio_scaled: vector, rawFeatures: vector, TF_IDF: vector, polarity_idx: double, features: vector, label: double, rawPrediction: vector, probability: vector, prediction: double, article_class: string]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0TjecAq_m5R",
        "outputId": "cdd2137d-06f0-4d05-9dd8-5ee99b04b17c"
      },
      "source": [
        "eval = MulticlassClassificationEvaluator(labelCol=\"label\",predictionCol=\"prediction\",metricName=\"accuracy\")\n",
        "acc_our_data_rfc = eval.evaluate(pred_rfc)\n",
        "print(\"our data: \", acc_our_data_rfc)\n",
        "acc_original_data_rfc = eval.evaluate(pred_original_rfc)\n",
        "print(\"original data: \", acc_original_data_rfc)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "our data:  0.841642228739003\n",
            "original data:  0.658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VylJyR2Y56yh"
      },
      "source": [
        ""
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F2NXdB_0_w_R",
        "outputId": "25970ae3-3fb0-48d9-9b9c-51993fc2b766"
      },
      "source": [
        "params = [{\n",
        "      p.name: v\n",
        "      for p,\n",
        "      v in m.items()\n",
        "   }\n",
        "   for m in cvModel_rfc.getEstimatorParamMaps()\n",
        "]\n",
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame.from_dict([{\n",
        "      cvModel_rfc.getEvaluator().getMetricName(): metric,\n",
        "      ** ps\n",
        "   } for ps, metric in zip(params, cvModel_rfc.avgMetrics)\n",
        "])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          f1  maxDepth  numFeatures  numTrees\n",
              "0   0.590927         3        10000        10\n",
              "1   0.566405         3        10000        20\n",
              "2   0.645885         3        10000        50\n",
              "3   0.632711         3        10000       100\n",
              "4   0.634485         5        10000        10\n",
              "5   0.637924         5        10000        20\n",
              "6   0.737245         5        10000        50\n",
              "7   0.740109         5        10000       100\n",
              "8   0.679977         9        10000        10\n",
              "9   0.705040         9        10000        20\n",
              "10  0.775312         9        10000        50\n",
              "11  0.787899         9        10000       100\n",
              "12  0.695226        15        10000        10\n",
              "13  0.747840        15        10000        20\n",
              "14  0.783808        15        10000        50\n",
              "15  0.809390        15        10000       100\n",
              "16  0.587368         3        20000        10\n",
              "17  0.612920         3        20000        20\n",
              "18  0.587036         3        20000        50\n",
              "19  0.598926         3        20000       100\n",
              "20  0.630892         5        20000        10\n",
              "21  0.663378         5        20000        20\n",
              "22  0.691563         5        20000        50\n",
              "23  0.681917         5        20000       100\n",
              "24  0.686031         9        20000        10\n",
              "25  0.726301         9        20000        20\n",
              "26  0.768266         9        20000        50\n",
              "27  0.762351         9        20000       100\n",
              "28  0.701408        15        20000        10\n",
              "29  0.737730        15        20000        20\n",
              "30  0.789012        15        20000        50\n",
              "31  0.797408        15        20000       100\n",
              "32  0.582513         3        50000        10\n",
              "33  0.568927         3        50000        20\n",
              "34  0.572879         3        50000        50\n",
              "35  0.504735         3        50000       100\n",
              "36  0.608606         5        50000        10\n",
              "37  0.661418         5        50000        20\n",
              "38  0.679428         5        50000        50\n",
              "39  0.634022         5        50000       100\n",
              "40  0.643019         9        50000        10\n",
              "41  0.712055         9        50000        20\n",
              "42  0.739895         9        50000        50\n",
              "43  0.733115         9        50000       100\n",
              "44  0.687773        15        50000        10\n",
              "45  0.743758        15        50000        20\n",
              "46  0.770905        15        50000        50\n",
              "47  0.769377        15        50000       100"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>maxDepth</th>\n",
              "      <th>numFeatures</th>\n",
              "      <th>numTrees</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.590927</td>\n",
              "      <td>3</td>\n",
              "      <td>10000</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.566405</td>\n",
              "      <td>3</td>\n",
              "      <td>10000</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.645885</td>\n",
              "      <td>3</td>\n",
              "      <td>10000</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.632711</td>\n",
              "      <td>3</td>\n",
              "      <td>10000</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.634485</td>\n",
              "      <td>5</td>\n",
              "      <td>10000</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.637924</td>\n",
              "      <td>5</td>\n",
              "      <td>10000</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.737245</td>\n",
              "      <td>5</td>\n",
              "      <td>10000</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.740109</td>\n",
              "      <td>5</td>\n",
              "      <td>10000</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.679977</td>\n",
              "      <td>9</td>\n",
              "      <td>10000</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.705040</td>\n",
              "      <td>9</td>\n",
              "      <td>10000</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.775312</td>\n",
              "      <td>9</td>\n",
              "      <td>10000</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.787899</td>\n",
              "      <td>9</td>\n",
              "      <td>10000</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.695226</td>\n",
              "      <td>15</td>\n",
              "      <td>10000</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.747840</td>\n",
              "      <td>15</td>\n",
              "      <td>10000</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.783808</td>\n",
              "      <td>15</td>\n",
              "      <td>10000</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.809390</td>\n",
              "      <td>15</td>\n",
              "      <td>10000</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.587368</td>\n",
              "      <td>3</td>\n",
              "      <td>20000</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.612920</td>\n",
              "      <td>3</td>\n",
              "      <td>20000</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.587036</td>\n",
              "      <td>3</td>\n",
              "      <td>20000</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.598926</td>\n",
              "      <td>3</td>\n",
              "      <td>20000</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.630892</td>\n",
              "      <td>5</td>\n",
              "      <td>20000</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.663378</td>\n",
              "      <td>5</td>\n",
              "      <td>20000</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.691563</td>\n",
              "      <td>5</td>\n",
              "      <td>20000</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.681917</td>\n",
              "      <td>5</td>\n",
              "      <td>20000</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.686031</td>\n",
              "      <td>9</td>\n",
              "      <td>20000</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.726301</td>\n",
              "      <td>9</td>\n",
              "      <td>20000</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.768266</td>\n",
              "      <td>9</td>\n",
              "      <td>20000</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.762351</td>\n",
              "      <td>9</td>\n",
              "      <td>20000</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.701408</td>\n",
              "      <td>15</td>\n",
              "      <td>20000</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.737730</td>\n",
              "      <td>15</td>\n",
              "      <td>20000</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.789012</td>\n",
              "      <td>15</td>\n",
              "      <td>20000</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.797408</td>\n",
              "      <td>15</td>\n",
              "      <td>20000</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.582513</td>\n",
              "      <td>3</td>\n",
              "      <td>50000</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.568927</td>\n",
              "      <td>3</td>\n",
              "      <td>50000</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.572879</td>\n",
              "      <td>3</td>\n",
              "      <td>50000</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.504735</td>\n",
              "      <td>3</td>\n",
              "      <td>50000</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.608606</td>\n",
              "      <td>5</td>\n",
              "      <td>50000</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.661418</td>\n",
              "      <td>5</td>\n",
              "      <td>50000</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.679428</td>\n",
              "      <td>5</td>\n",
              "      <td>50000</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.634022</td>\n",
              "      <td>5</td>\n",
              "      <td>50000</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.643019</td>\n",
              "      <td>9</td>\n",
              "      <td>50000</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.712055</td>\n",
              "      <td>9</td>\n",
              "      <td>50000</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.739895</td>\n",
              "      <td>9</td>\n",
              "      <td>50000</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.733115</td>\n",
              "      <td>9</td>\n",
              "      <td>50000</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.687773</td>\n",
              "      <td>15</td>\n",
              "      <td>50000</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.743758</td>\n",
              "      <td>15</td>\n",
              "      <td>50000</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.770905</td>\n",
              "      <td>15</td>\n",
              "      <td>50000</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.769377</td>\n",
              "      <td>15</td>\n",
              "      <td>50000</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0NiNweqAN8W"
      },
      "source": [
        "#greg\n",
        "\n",
        "from pyspark.ml.feature import HashingTF, IDF, IndexToString, StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "train_split, test_split = df_train.randomSplit(weights = [0.80, 0.20], seed = 1)\n",
        "train_split.cache()\n",
        "\n",
        "columns_to_be_scaled = ['length_in_words', 'average_word_length', 'capital_char_ratio', 'long_word_ratio', 'non_stop_word_ratio', 'punctuation_ratio']\n",
        "assemblers = [VectorAssembler(inputCols=[col], outputCol=col + \"_vec\") for col in columns_to_be_scaled]\n",
        "scalers  = [StandardScaler(inputCol=col + \"_vec\", outputCol=col + \"_scaled\") for col in columns_to_be_scaled]\n",
        "scaling_pipeline = Pipeline(stages=assemblers + scalers)\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"stemmed\", outputCol=\"rawFeatures\")\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"TF_IDF\", minDocFreq=2)\n",
        "label_strIdx1 = StringIndexer(inputCol=\"polarity\", outputCol=\"polarity_idx\")\n",
        "assembles = VectorAssembler(inputCols = ['TF_IDF','sentiment','polarity_idx'],outputCol=\"features\")\n",
        "label_strIdx2 = StringIndexer(inputCol=\"real_fake\", outputCol=\"label\")\n",
        "gbtc = GBTClassifier(featuresCol=\"features\",labelCol=\"label\")\n",
        "label_idxStr = IndexToString(inputCol = \"label\", outputCol = \"article_class\")\n",
        "\n",
        "pipeline_gbtc = Pipeline(stages=[scaling_pipeline, hashingTF, idf,label_strIdx1 , assembles, label_strIdx2, gbtc ,label_idxStr])"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aEGVcyLBy51"
      },
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(hashingTF.numFeatures, [10000,20000,50000]) \\\n",
        "    .addGrid(gbtc.maxDepth, [3, 5, 9]) \\\n",
        "    .addGrid(gbtc.stepSize, [0.1, 0.5, 1]) \\\n",
        "    .build()\n",
        "crossval_gbtc = CrossValidator(estimator=pipeline_gbtc, estimatorParamMaps=paramGrid,evaluator= MulticlassClassificationEvaluator(),numFolds=2,parallelism = 100 )  # use 3+ folds in practice"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "1eb-J_DdCq1I",
        "outputId": "f6008a64-2e79-45b2-8a35-e23bcc755215"
      },
      "source": [
        "cvModel_gbtc = crossval_gbtc.fit(train_split)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-e44f8fde2db7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcvModel_gbtc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrossval_gbtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/global/software/jupyterhub-spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/global/software/jupyterhub-spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parallelFitTasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnFolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/global/software/jupyterhub-spark/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0m__next__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m                    \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/global/software/jupyterhub-spark/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/global/software/jupyterhub-spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parallelFitTasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnFolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/global/software/jupyterhub-spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36msingleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msingleTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelIter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModel\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/global/software/jupyterhub-spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No models remaining.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitSingleModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/global/software/jupyterhub-spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfitSingleModel\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfitSingleModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparamMaps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_FitMultipleIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfitSingleModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparamMaps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/global/software/jupyterhub-spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/global/software/jupyterhub-spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/global/software/jupyterhub-spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/global/software/jupyterhub-spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/global/software/jupyterhub-spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/global/software/jupyterhub-spark/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/global/software/jupyterhub-spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/global/software/jupyterhub-spark/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o346753.fit.\n: java.lang.OutOfMemoryError: Java heap space\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdH_7sRnC6Dg"
      },
      "source": [
        "pred_gbtc = cvModel_gbtc.transform(test_split)\n",
        "pred_original_gbtc = cvModel_gbtc.transform(df_test)\n",
        "pred_gbtc.cache()\n",
        "pred_original_gbtc.cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_91JR4vDC-xA"
      },
      "source": [
        "eval = MulticlassClassificationEvaluator(labelCol=\"label\",predictionCol=\"prediction\",metricName=\"accuracy\")\n",
        "acc_our_data_gbtc = eval.evaluate(pred_gbtc)\n",
        "print(\"our data: \", acc_our_data_gbtc)\n",
        "acc_original_data_gbtc = eval.evaluate(pred_original_gbtc)\n",
        "print(\"original data: \", acc_original_data_gbtc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsxqI7FYDKtv"
      },
      "source": [
        "params = [{\n",
        "      p.name: v\n",
        "      for p,\n",
        "      v in m.items()\n",
        "   }\n",
        "   for m in cvModel_gbtc.getEstimatorParamMaps()\n",
        "]\n",
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame.from_dict([{\n",
        "      cvModel_gbtc.getEvaluator().getMetricName(): metric,\n",
        "      ** ps\n",
        "   }\n",
        "   for ps, metric in zip(params, cvModel_gbtc.avgMetrics)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DaBrnnNDeJe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}